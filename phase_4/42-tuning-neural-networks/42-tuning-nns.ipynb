{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 42: Transfer Learning and Tuning Neural Networks\n",
    "\n",
    "1. Using GridSearch/Talos for finding optimal parameter combinations \n",
    "2. Saving your neural network to disk\n",
    "4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tuning hyperparameters with Keras Models\n",
    "\n",
    "There are a couple ways to go about testing combinations of parameters, GridSearch style:\n",
    "* **Using SKlearn GridSearch**: https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/\n",
    "    * This involves creating a model object such that scikit-learn's existing GridSearch functions work with your neural net.\n",
    "* **Using KerasTuner**: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "* **Using Talos**: https://autonomio.github.io/talos/#/Scan\n",
    "    * This library lets you tune without having to create the model object, and also can automatically output your parameter combination scores into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install talos\n",
    "import talos\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "Let's return to our Seattle Housing data from the last study group.\n",
    "\n",
    "We are going to split the testing set into a validation and a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_98148</th>\n",
       "      <th>zipcode_98155</th>\n",
       "      <th>zipcode_98166</th>\n",
       "      <th>zipcode_98168</th>\n",
       "      <th>zipcode_98177</th>\n",
       "      <th>zipcode_98178</th>\n",
       "      <th>zipcode_98188</th>\n",
       "      <th>zipcode_98198</th>\n",
       "      <th>zipcode_98199</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>-0.479166</td>\n",
       "      <td>-1.044407</td>\n",
       "      <td>-0.266958</td>\n",
       "      <td>-0.917244</td>\n",
       "      <td>0.907485</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>-1.106775</td>\n",
       "      <td>-1.875431</td>\n",
       "      <td>0.781814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>-0.479166</td>\n",
       "      <td>-0.547215</td>\n",
       "      <td>-0.204171</td>\n",
       "      <td>-0.917244</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>-0.254565</td>\n",
       "      <td>0.574854</td>\n",
       "      <td>-2.019417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>-0.801723</td>\n",
       "      <td>-0.320235</td>\n",
       "      <td>-0.182459</td>\n",
       "      <td>-0.917244</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>-0.002503</td>\n",
       "      <td>-0.616257</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>865000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>0.488506</td>\n",
       "      <td>-0.104064</td>\n",
       "      <td>-0.267341</td>\n",
       "      <td>0.937512</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>0.288544</td>\n",
       "      <td>0.237556</td>\n",
       "      <td>1.119362</td>\n",
       "      <td>0.226758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.669418</td>\n",
       "      <td>0.488506</td>\n",
       "      <td>-0.071639</td>\n",
       "      <td>-0.213373</td>\n",
       "      <td>0.937512</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>0.273564</td>\n",
       "      <td>1.051299</td>\n",
       "      <td>-2.020859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16192</th>\n",
       "      <td>-1.474441</td>\n",
       "      <td>-1.446837</td>\n",
       "      <td>-1.238961</td>\n",
       "      <td>-0.178290</td>\n",
       "      <td>-0.917244</td>\n",
       "      <td>0.907485</td>\n",
       "      <td>-1.415265</td>\n",
       "      <td>-1.022754</td>\n",
       "      <td>-1.875431</td>\n",
       "      <td>1.057900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16193</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>0.488506</td>\n",
       "      <td>0.425554</td>\n",
       "      <td>0.544071</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.907485</td>\n",
       "      <td>0.288544</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.098410</td>\n",
       "      <td>-1.150790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>663000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16194</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>-0.479166</td>\n",
       "      <td>-0.817428</td>\n",
       "      <td>-0.060432</td>\n",
       "      <td>-0.917244</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>-0.554639</td>\n",
       "      <td>0.404696</td>\n",
       "      <td>-0.862449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16195</th>\n",
       "      <td>-0.402511</td>\n",
       "      <td>-0.479166</td>\n",
       "      <td>-0.098660</td>\n",
       "      <td>0.077172</td>\n",
       "      <td>0.937512</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>-0.563361</td>\n",
       "      <td>0.243557</td>\n",
       "      <td>-0.480130</td>\n",
       "      <td>-1.059963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16196</th>\n",
       "      <td>0.669418</td>\n",
       "      <td>0.165949</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>-0.302905</td>\n",
       "      <td>2.792268</td>\n",
       "      <td>-0.629858</td>\n",
       "      <td>0.288544</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>1.017267</td>\n",
       "      <td>0.812090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16197 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bedrooms  bathrooms  sqft_living  sqft_lot    floors  condition  \\\n",
       "0     -0.402511  -0.479166    -1.044407 -0.266958 -0.917244   0.907485   \n",
       "1     -0.402511  -0.479166    -0.547215 -0.204171 -0.917244  -0.629858   \n",
       "2     -0.402511  -0.801723    -0.320235 -0.182459 -0.917244  -0.629858   \n",
       "3     -0.402511   0.488506    -0.104064 -0.267341  0.937512  -0.629858   \n",
       "4      0.669418   0.488506    -0.071639 -0.213373  0.937512  -0.629858   \n",
       "...         ...        ...          ...       ...       ...        ...   \n",
       "16192 -1.474441  -1.446837    -1.238961 -0.178290 -0.917244   0.907485   \n",
       "16193 -0.402511   0.488506     0.425554  0.544071  0.010134   0.907485   \n",
       "16194 -0.402511  -0.479166    -0.817428 -0.060432 -0.917244  -0.629858   \n",
       "16195 -0.402511  -0.479166    -0.098660  0.077172  0.937512  -0.629858   \n",
       "16196  0.669418   0.165949     0.014829 -0.302905  2.792268  -0.629858   \n",
       "\n",
       "          grade  sqft_above  yr_built       lat  ...  zipcode_98148  \\\n",
       "0     -0.563361   -1.106775 -1.875431  0.781814  ...            0.0   \n",
       "1     -0.563361   -0.254565  0.574854 -2.019417  ...            0.0   \n",
       "2     -0.563361   -0.002503 -0.616257  0.567721  ...            0.0   \n",
       "3      0.288544    0.237556  1.119362  0.226758  ...            0.0   \n",
       "4     -0.563361    0.273564  1.051299 -2.020859  ...            0.0   \n",
       "...         ...         ...       ...       ...  ...            ...   \n",
       "16192 -1.415265   -1.022754 -1.875431  1.057900  ...            0.0   \n",
       "16193  0.288544    0.825700  0.098410 -1.150790  ...            0.0   \n",
       "16194 -0.563361   -0.554639  0.404696 -0.862449  ...            0.0   \n",
       "16195 -0.563361    0.243557 -0.480130 -1.059963  ...            0.0   \n",
       "16196  0.288544    0.369588  1.017267  0.812090  ...            0.0   \n",
       "\n",
       "       zipcode_98155  zipcode_98166  zipcode_98168  zipcode_98177  \\\n",
       "0                0.0            0.0            0.0            0.0   \n",
       "1                0.0            0.0            0.0            0.0   \n",
       "2                0.0            0.0            0.0            0.0   \n",
       "3                0.0            0.0            0.0            0.0   \n",
       "4                0.0            0.0            0.0            0.0   \n",
       "...              ...            ...            ...            ...   \n",
       "16192            0.0            0.0            0.0            0.0   \n",
       "16193            0.0            0.0            0.0            0.0   \n",
       "16194            0.0            0.0            0.0            0.0   \n",
       "16195            0.0            0.0            0.0            0.0   \n",
       "16196            0.0            0.0            0.0            0.0   \n",
       "\n",
       "       zipcode_98178  zipcode_98188  zipcode_98198  zipcode_98199     price  \n",
       "0                0.0            0.0            0.0            0.0  445000.0  \n",
       "1                0.0            0.0            0.0            0.0  279900.0  \n",
       "2                0.0            0.0            0.0            0.0  865000.0  \n",
       "3                0.0            0.0            0.0            0.0  560200.0  \n",
       "4                0.0            0.0            0.0            0.0  262000.0  \n",
       "...              ...            ...            ...            ...       ...  \n",
       "16192            0.0            0.0            0.0            0.0  329000.0  \n",
       "16193            0.0            0.0            0.0            0.0  663000.0  \n",
       "16194            0.0            1.0            0.0            0.0  265000.0  \n",
       "16195            0.0            0.0            1.0            0.0  271500.0  \n",
       "16196            0.0            0.0            0.0            0.0  650000.0  \n",
       "\n",
       "[16197 rows x 88 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16197 entries, 0 to 16196\n",
      "Data columns (total 88 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   bedrooms       16197 non-null  float64\n",
      " 1   bathrooms      16197 non-null  float64\n",
      " 2   sqft_living    16197 non-null  float64\n",
      " 3   sqft_lot       16197 non-null  float64\n",
      " 4   floors         16197 non-null  float64\n",
      " 5   condition      16197 non-null  float64\n",
      " 6   grade          16197 non-null  float64\n",
      " 7   sqft_above     16197 non-null  float64\n",
      " 8   yr_built       16197 non-null  float64\n",
      " 9   lat            16197 non-null  float64\n",
      " 10  long           16197 non-null  float64\n",
      " 11  sqft_living15  16197 non-null  float64\n",
      " 12  sqft_lot15     16197 non-null  float64\n",
      " 13  view           16197 non-null  float64\n",
      " 14  sqft_basement  16197 non-null  float64\n",
      " 15  yr_renovated   16197 non-null  float64\n",
      " 16  waterfront     16197 non-null  float64\n",
      " 17  zipcode_98001  16197 non-null  float64\n",
      " 18  zipcode_98002  16197 non-null  float64\n",
      " 19  zipcode_98003  16197 non-null  float64\n",
      " 20  zipcode_98004  16197 non-null  float64\n",
      " 21  zipcode_98005  16197 non-null  float64\n",
      " 22  zipcode_98006  16197 non-null  float64\n",
      " 23  zipcode_98007  16197 non-null  float64\n",
      " 24  zipcode_98008  16197 non-null  float64\n",
      " 25  zipcode_98010  16197 non-null  float64\n",
      " 26  zipcode_98011  16197 non-null  float64\n",
      " 27  zipcode_98014  16197 non-null  float64\n",
      " 28  zipcode_98019  16197 non-null  float64\n",
      " 29  zipcode_98022  16197 non-null  float64\n",
      " 30  zipcode_98023  16197 non-null  float64\n",
      " 31  zipcode_98024  16197 non-null  float64\n",
      " 32  zipcode_98027  16197 non-null  float64\n",
      " 33  zipcode_98028  16197 non-null  float64\n",
      " 34  zipcode_98029  16197 non-null  float64\n",
      " 35  zipcode_98030  16197 non-null  float64\n",
      " 36  zipcode_98031  16197 non-null  float64\n",
      " 37  zipcode_98032  16197 non-null  float64\n",
      " 38  zipcode_98033  16197 non-null  float64\n",
      " 39  zipcode_98034  16197 non-null  float64\n",
      " 40  zipcode_98038  16197 non-null  float64\n",
      " 41  zipcode_98039  16197 non-null  float64\n",
      " 42  zipcode_98040  16197 non-null  float64\n",
      " 43  zipcode_98042  16197 non-null  float64\n",
      " 44  zipcode_98045  16197 non-null  float64\n",
      " 45  zipcode_98052  16197 non-null  float64\n",
      " 46  zipcode_98053  16197 non-null  float64\n",
      " 47  zipcode_98055  16197 non-null  float64\n",
      " 48  zipcode_98056  16197 non-null  float64\n",
      " 49  zipcode_98058  16197 non-null  float64\n",
      " 50  zipcode_98059  16197 non-null  float64\n",
      " 51  zipcode_98065  16197 non-null  float64\n",
      " 52  zipcode_98070  16197 non-null  float64\n",
      " 53  zipcode_98072  16197 non-null  float64\n",
      " 54  zipcode_98074  16197 non-null  float64\n",
      " 55  zipcode_98075  16197 non-null  float64\n",
      " 56  zipcode_98077  16197 non-null  float64\n",
      " 57  zipcode_98092  16197 non-null  float64\n",
      " 58  zipcode_98102  16197 non-null  float64\n",
      " 59  zipcode_98103  16197 non-null  float64\n",
      " 60  zipcode_98105  16197 non-null  float64\n",
      " 61  zipcode_98106  16197 non-null  float64\n",
      " 62  zipcode_98107  16197 non-null  float64\n",
      " 63  zipcode_98108  16197 non-null  float64\n",
      " 64  zipcode_98109  16197 non-null  float64\n",
      " 65  zipcode_98112  16197 non-null  float64\n",
      " 66  zipcode_98115  16197 non-null  float64\n",
      " 67  zipcode_98116  16197 non-null  float64\n",
      " 68  zipcode_98117  16197 non-null  float64\n",
      " 69  zipcode_98118  16197 non-null  float64\n",
      " 70  zipcode_98119  16197 non-null  float64\n",
      " 71  zipcode_98122  16197 non-null  float64\n",
      " 72  zipcode_98125  16197 non-null  float64\n",
      " 73  zipcode_98126  16197 non-null  float64\n",
      " 74  zipcode_98133  16197 non-null  float64\n",
      " 75  zipcode_98136  16197 non-null  float64\n",
      " 76  zipcode_98144  16197 non-null  float64\n",
      " 77  zipcode_98146  16197 non-null  float64\n",
      " 78  zipcode_98148  16197 non-null  float64\n",
      " 79  zipcode_98155  16197 non-null  float64\n",
      " 80  zipcode_98166  16197 non-null  float64\n",
      " 81  zipcode_98168  16197 non-null  float64\n",
      " 82  zipcode_98177  16197 non-null  float64\n",
      " 83  zipcode_98178  16197 non-null  float64\n",
      " 84  zipcode_98188  16197 non-null  float64\n",
      " 85  zipcode_98198  16197 non-null  float64\n",
      " 86  zipcode_98199  16197 non-null  float64\n",
      " 87  price          16197 non-null  float64\n",
      "dtypes: float64(88)\n",
      "memory usage: 10.9 MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/011121-pt-ds/main/phase_4/advanced-neural-networks/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/011121-pt-ds/main/phase_4/advanced-neural-networks/test.csv')\n",
    "split = int(len(test)*.5)\n",
    "val = test.iloc[:split, :]\n",
    "holdout = test.iloc[split:, :]\n",
    "\n",
    "X_train, y_train = (train.drop('price', axis=1), train['price'])\n",
    "X_val, y_val = (val.drop('price', axis=1), val['price'])\n",
    "X_holdout, y_holdout = (holdout.drop('price', axis=1), holdout['price'])\n",
    "\n",
    "display(train)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Function to Create a Model\n",
    "\n",
    "To work with Talos, we create a specially formed function that returns our model and the model training history.\n",
    "\n",
    "The function must take: `X_train, y_train, X_test, y_test, params` in that order.  Params is a dictionary of parameters.  Keys should be a name for the hyperparameter.  It is arbitrary, but should be something descriptive.  The values are the range of values that hyperparameter could take.\n",
    "\n",
    "Talos will pass only one of the possible values for each hyperparameter to the function on each experiment in the `params` dictionary that the function expects.\n",
    "\n",
    "We then set the hyperparameters of the model generated in the function to be the value we want to try from the params dictionary.\n",
    "\n",
    "Another note: we can adjust the depth of the model by adding layers in a loop, a series of loops, or even nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_network(x_train, y_train, x_test, y_test, params):\n",
    "\n",
    "    #we build the model like we would normally do it\n",
    "    model = Sequential()\n",
    "    \n",
    "    #input layer\n",
    "    model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # hidden layers\n",
    "    for layer in range(params['dense_layers']):\n",
    "        model.add(Dense(params['nodes'], activation=params['activation']))\n",
    "        model.add(layers.Dropout(params['dropout']))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=params['optimizer'], metrics=['mae', 'mse'])\n",
    "    \n",
    "    #callback to prevent over-training\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    out = model.fit(x_train, y_train, \n",
    "                   validation_data=(x_test, y_test),\n",
    "                   batch_size=50,\n",
    "                   epochs=10,\n",
    "                   verbose=0,\n",
    "                   callbacks = [earlystopping])\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:\n",
    "\n",
    "Define a dictionary of possible parameter values.  Remember that more parameters quickly multiply the number of models Talos will compare.  It's okay to do a series of experiments based on the results of your previous ones, rather than on big gridsearch.\n",
    "\n",
    "Alternatively you can use [Probabalistic Reduction](https://autonomio.github.io/talos/#/Probabilistic_Reduction) to instruct Scan to keep hyperparameter values that have shown to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dropout': [0.2, 0.5], \n",
    "          'optimizer': ['adam', 'sgd'], \n",
    "          'activation': ['relu', 'tanh'], \n",
    "          'dense_layers': [5,10],\n",
    "          'nodes': [100,200]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`talos.Scan()` instantiates a new scan object by conducting all of the experiments defined in `params` using the model defined in the `model=` argument and the data passed.\n",
    "\n",
    "your scan object (named `results` below) will contain the record of your experiments, including all of the fitted models.  It also saves records to disk under the folder defined in `experiment_name=`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [03:42<00:00,  6.97s/it]\n"
     ]
    }
   ],
   "source": [
    "results = talos.Scan(X_train, y_train, \n",
    "                     x_val=X_val,\n",
    "                     y_val=y_val,\n",
    "                     params=params, \n",
    "                     model=dense_network,\n",
    "                     experiment_name='grid',\n",
    "                     minimize_loss=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Record\n",
    "\n",
    "The record of all experiments is stored in a dataframe in the `.data` attribute.  \n",
    "\n",
    "The scan object will keep a record of scores according to the metrics defined when the model was compiled.  You can use these metrics to order the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>activation</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>nodes</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/03/21-152448</td>\n",
       "      <td>08/03/21-152458</td>\n",
       "      <td>9.823584</td>\n",
       "      <td>10</td>\n",
       "      <td>2.144835e+10</td>\n",
       "      <td>90865.343750</td>\n",
       "      <td>2.144835e+10</td>\n",
       "      <td>1.389579e+10</td>\n",
       "      <td>75770.218750</td>\n",
       "      <td>1.389579e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/03/21-152439</td>\n",
       "      <td>08/03/21-152445</td>\n",
       "      <td>6.439806</td>\n",
       "      <td>10</td>\n",
       "      <td>2.677377e+10</td>\n",
       "      <td>100079.570312</td>\n",
       "      <td>2.677377e+10</td>\n",
       "      <td>1.526509e+10</td>\n",
       "      <td>78941.773438</td>\n",
       "      <td>1.526509e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08/03/21-152520</td>\n",
       "      <td>08/03/21-152529</td>\n",
       "      <td>9.569853</td>\n",
       "      <td>10</td>\n",
       "      <td>3.657275e+10</td>\n",
       "      <td>120794.320312</td>\n",
       "      <td>3.657274e+10</td>\n",
       "      <td>1.645498e+10</td>\n",
       "      <td>80395.109375</td>\n",
       "      <td>1.645498e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08/03/21-152509</td>\n",
       "      <td>08/03/21-152517</td>\n",
       "      <td>8.088457</td>\n",
       "      <td>8</td>\n",
       "      <td>3.464387e+10</td>\n",
       "      <td>117413.835938</td>\n",
       "      <td>3.464387e+10</td>\n",
       "      <td>1.621678e+10</td>\n",
       "      <td>82495.007812</td>\n",
       "      <td>1.621678e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/03/21-152501</td>\n",
       "      <td>08/03/21-152507</td>\n",
       "      <td>6.242362</td>\n",
       "      <td>10</td>\n",
       "      <td>4.506420e+10</td>\n",
       "      <td>135867.921875</td>\n",
       "      <td>4.506420e+10</td>\n",
       "      <td>1.726049e+10</td>\n",
       "      <td>83724.015625</td>\n",
       "      <td>1.726049e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>08/03/21-152532</td>\n",
       "      <td>08/03/21-152547</td>\n",
       "      <td>15.117230</td>\n",
       "      <td>9</td>\n",
       "      <td>2.975040e+10</td>\n",
       "      <td>108701.460938</td>\n",
       "      <td>2.975039e+10</td>\n",
       "      <td>2.763869e+10</td>\n",
       "      <td>125190.648438</td>\n",
       "      <td>2.763868e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>08/03/21-152643</td>\n",
       "      <td>08/03/21-152646</td>\n",
       "      <td>3.114382</td>\n",
       "      <td>4</td>\n",
       "      <td>1.428123e+11</td>\n",
       "      <td>239281.156250</td>\n",
       "      <td>1.428123e+11</td>\n",
       "      <td>8.732812e+10</td>\n",
       "      <td>189113.937500</td>\n",
       "      <td>8.732812e+10</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>08/03/21-152600</td>\n",
       "      <td>08/03/21-152609</td>\n",
       "      <td>9.152887</td>\n",
       "      <td>5</td>\n",
       "      <td>6.431127e+10</td>\n",
       "      <td>163381.578125</td>\n",
       "      <td>6.431127e+10</td>\n",
       "      <td>6.110067e+10</td>\n",
       "      <td>195479.671875</td>\n",
       "      <td>6.110067e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>08/03/21-152656</td>\n",
       "      <td>08/03/21-152704</td>\n",
       "      <td>8.040211</td>\n",
       "      <td>9</td>\n",
       "      <td>1.495984e+11</td>\n",
       "      <td>247378.328125</td>\n",
       "      <td>1.495984e+11</td>\n",
       "      <td>1.035998e+11</td>\n",
       "      <td>208676.718750</td>\n",
       "      <td>1.035998e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>08/03/21-152715</td>\n",
       "      <td>08/03/21-152720</td>\n",
       "      <td>5.534511</td>\n",
       "      <td>7</td>\n",
       "      <td>1.511078e+11</td>\n",
       "      <td>246557.171875</td>\n",
       "      <td>1.511078e+11</td>\n",
       "      <td>1.118859e+11</td>\n",
       "      <td>212283.421875</td>\n",
       "      <td>1.118859e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>08/03/21-152552</td>\n",
       "      <td>08/03/21-152557</td>\n",
       "      <td>5.529936</td>\n",
       "      <td>6</td>\n",
       "      <td>8.299420e+10</td>\n",
       "      <td>184632.406250</td>\n",
       "      <td>8.299420e+10</td>\n",
       "      <td>8.165568e+10</td>\n",
       "      <td>217551.875000</td>\n",
       "      <td>8.165568e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>08/03/21-152753</td>\n",
       "      <td>08/03/21-152758</td>\n",
       "      <td>4.642022</td>\n",
       "      <td>6</td>\n",
       "      <td>1.481313e+11</td>\n",
       "      <td>243918.390625</td>\n",
       "      <td>1.481313e+11</td>\n",
       "      <td>1.106875e+11</td>\n",
       "      <td>232091.109375</td>\n",
       "      <td>1.106875e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>08/03/21-152620</td>\n",
       "      <td>08/03/21-152623</td>\n",
       "      <td>3.120467</td>\n",
       "      <td>6</td>\n",
       "      <td>1.416299e+11</td>\n",
       "      <td>238575.859375</td>\n",
       "      <td>1.416299e+11</td>\n",
       "      <td>1.030976e+11</td>\n",
       "      <td>234851.562500</td>\n",
       "      <td>1.030976e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>08/03/21-152816</td>\n",
       "      <td>08/03/21-152822</td>\n",
       "      <td>5.356652</td>\n",
       "      <td>3</td>\n",
       "      <td>1.560586e+11</td>\n",
       "      <td>253775.859375</td>\n",
       "      <td>1.560586e+11</td>\n",
       "      <td>1.176267e+11</td>\n",
       "      <td>241852.796875</td>\n",
       "      <td>1.176267e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>08/03/21-152633</td>\n",
       "      <td>08/03/21-152637</td>\n",
       "      <td>3.320946</td>\n",
       "      <td>3</td>\n",
       "      <td>2.721428e+11</td>\n",
       "      <td>389014.781250</td>\n",
       "      <td>2.721428e+11</td>\n",
       "      <td>1.317893e+11</td>\n",
       "      <td>257560.718750</td>\n",
       "      <td>1.317893e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>08/03/21-152739</td>\n",
       "      <td>08/03/21-152744</td>\n",
       "      <td>5.378550</td>\n",
       "      <td>3</td>\n",
       "      <td>4.158681e+11</td>\n",
       "      <td>519540.218750</td>\n",
       "      <td>4.158681e+11</td>\n",
       "      <td>3.807378e+11</td>\n",
       "      <td>521109.156250</td>\n",
       "      <td>3.807378e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>08/03/21-152623</td>\n",
       "      <td>08/03/21-152633</td>\n",
       "      <td>9.954805</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358172e+11</td>\n",
       "      <td>542045.187500</td>\n",
       "      <td>4.358172e+11</td>\n",
       "      <td>3.865410e+11</td>\n",
       "      <td>527837.625000</td>\n",
       "      <td>3.865410e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>08/03/21-152646</td>\n",
       "      <td>08/03/21-152656</td>\n",
       "      <td>9.653022</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358254e+11</td>\n",
       "      <td>542052.875000</td>\n",
       "      <td>4.358255e+11</td>\n",
       "      <td>3.865486e+11</td>\n",
       "      <td>527844.875000</td>\n",
       "      <td>3.865486e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>08/03/21-152758</td>\n",
       "      <td>08/03/21-152816</td>\n",
       "      <td>17.764553</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358260e+11</td>\n",
       "      <td>542053.500000</td>\n",
       "      <td>4.358261e+11</td>\n",
       "      <td>3.865487e+11</td>\n",
       "      <td>527844.875000</td>\n",
       "      <td>3.865487e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>08/03/21-152720</td>\n",
       "      <td>08/03/21-152739</td>\n",
       "      <td>18.373596</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358985e+11</td>\n",
       "      <td>542105.500000</td>\n",
       "      <td>4.358985e+11</td>\n",
       "      <td>3.866162e+11</td>\n",
       "      <td>527898.312500</td>\n",
       "      <td>3.866162e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>08/03/21-152614</td>\n",
       "      <td>08/03/21-152619</td>\n",
       "      <td>5.930509</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361600e+11</td>\n",
       "      <td>542361.250000</td>\n",
       "      <td>4.361600e+11</td>\n",
       "      <td>3.868919e+11</td>\n",
       "      <td>528169.875000</td>\n",
       "      <td>3.868919e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>08/03/21-152745</td>\n",
       "      <td>08/03/21-152753</td>\n",
       "      <td>8.598542</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361619e+11</td>\n",
       "      <td>542362.812500</td>\n",
       "      <td>4.361619e+11</td>\n",
       "      <td>3.868937e+11</td>\n",
       "      <td>528171.687500</td>\n",
       "      <td>3.868937e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>08/03/21-152637</td>\n",
       "      <td>08/03/21-152643</td>\n",
       "      <td>6.008372</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361628e+11</td>\n",
       "      <td>542363.562500</td>\n",
       "      <td>4.361628e+11</td>\n",
       "      <td>3.868939e+11</td>\n",
       "      <td>528171.875000</td>\n",
       "      <td>3.868939e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>08/03/21-152705</td>\n",
       "      <td>08/03/21-152714</td>\n",
       "      <td>9.828249</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361642e+11</td>\n",
       "      <td>542365.437500</td>\n",
       "      <td>4.361642e+11</td>\n",
       "      <td>3.868946e+11</td>\n",
       "      <td>528172.500000</td>\n",
       "      <td>3.868946e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/03/21-152446</td>\n",
       "      <td>08/03/21-152448</td>\n",
       "      <td>1.890808</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/03/21-152458</td>\n",
       "      <td>08/03/21-152501</td>\n",
       "      <td>2.411776</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08/03/21-152507</td>\n",
       "      <td>08/03/21-152509</td>\n",
       "      <td>1.559266</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08/03/21-152517</td>\n",
       "      <td>08/03/21-152520</td>\n",
       "      <td>2.318742</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>08/03/21-152530</td>\n",
       "      <td>08/03/21-152532</td>\n",
       "      <td>2.324288</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08/03/21-152548</td>\n",
       "      <td>08/03/21-152551</td>\n",
       "      <td>3.728953</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>08/03/21-152557</td>\n",
       "      <td>08/03/21-152600</td>\n",
       "      <td>2.183546</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>08/03/21-152609</td>\n",
       "      <td>08/03/21-152613</td>\n",
       "      <td>4.137468</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              start              end   duration  round_epochs          loss  \\\n",
       "2   08/03/21-152448  08/03/21-152458   9.823584            10  2.144835e+10   \n",
       "0   08/03/21-152439  08/03/21-152445   6.439806            10  2.677377e+10   \n",
       "8   08/03/21-152520  08/03/21-152529   9.569853            10  3.657275e+10   \n",
       "6   08/03/21-152509  08/03/21-152517   8.088457             8  3.464387e+10   \n",
       "4   08/03/21-152501  08/03/21-152507   6.242362            10  4.506420e+10   \n",
       "10  08/03/21-152532  08/03/21-152547  15.117230             9  2.975040e+10   \n",
       "21  08/03/21-152643  08/03/21-152646   3.114382             4  1.428123e+11   \n",
       "14  08/03/21-152600  08/03/21-152609   9.152887             5  6.431127e+10   \n",
       "23  08/03/21-152656  08/03/21-152704   8.040211             9  1.495984e+11   \n",
       "25  08/03/21-152715  08/03/21-152720   5.534511             7  1.511078e+11   \n",
       "12  08/03/21-152552  08/03/21-152557   5.529936             6  8.299420e+10   \n",
       "29  08/03/21-152753  08/03/21-152758   4.642022             6  1.481313e+11   \n",
       "17  08/03/21-152620  08/03/21-152623   3.120467             6  1.416299e+11   \n",
       "31  08/03/21-152816  08/03/21-152822   5.356652             3  1.560586e+11   \n",
       "19  08/03/21-152633  08/03/21-152637   3.320946             3  2.721428e+11   \n",
       "27  08/03/21-152739  08/03/21-152744   5.378550             3  4.158681e+11   \n",
       "18  08/03/21-152623  08/03/21-152633   9.954805            10  4.358172e+11   \n",
       "22  08/03/21-152646  08/03/21-152656   9.653022            10  4.358254e+11   \n",
       "30  08/03/21-152758  08/03/21-152816  17.764553            10  4.358260e+11   \n",
       "26  08/03/21-152720  08/03/21-152739  18.373596            10  4.358985e+11   \n",
       "16  08/03/21-152614  08/03/21-152619   5.930509            10  4.361600e+11   \n",
       "28  08/03/21-152745  08/03/21-152753   8.598542            10  4.361619e+11   \n",
       "20  08/03/21-152637  08/03/21-152643   6.008372            10  4.361628e+11   \n",
       "24  08/03/21-152705  08/03/21-152714   9.828249            10  4.361642e+11   \n",
       "1   08/03/21-152446  08/03/21-152448   1.890808             2           NaN   \n",
       "3   08/03/21-152458  08/03/21-152501   2.411776             2           NaN   \n",
       "5   08/03/21-152507  08/03/21-152509   1.559266             2           NaN   \n",
       "7   08/03/21-152517  08/03/21-152520   2.318742             2           NaN   \n",
       "9   08/03/21-152530  08/03/21-152532   2.324288             2           NaN   \n",
       "11  08/03/21-152548  08/03/21-152551   3.728953             2           NaN   \n",
       "13  08/03/21-152557  08/03/21-152600   2.183546             2           NaN   \n",
       "15  08/03/21-152609  08/03/21-152613   4.137468             2           NaN   \n",
       "\n",
       "              mae           mse      val_loss        val_mae       val_mse  \\\n",
       "2    90865.343750  2.144835e+10  1.389579e+10   75770.218750  1.389579e+10   \n",
       "0   100079.570312  2.677377e+10  1.526509e+10   78941.773438  1.526509e+10   \n",
       "8   120794.320312  3.657274e+10  1.645498e+10   80395.109375  1.645498e+10   \n",
       "6   117413.835938  3.464387e+10  1.621678e+10   82495.007812  1.621678e+10   \n",
       "4   135867.921875  4.506420e+10  1.726049e+10   83724.015625  1.726049e+10   \n",
       "10  108701.460938  2.975039e+10  2.763869e+10  125190.648438  2.763868e+10   \n",
       "21  239281.156250  1.428123e+11  8.732812e+10  189113.937500  8.732812e+10   \n",
       "14  163381.578125  6.431127e+10  6.110067e+10  195479.671875  6.110067e+10   \n",
       "23  247378.328125  1.495984e+11  1.035998e+11  208676.718750  1.035998e+11   \n",
       "25  246557.171875  1.511078e+11  1.118859e+11  212283.421875  1.118859e+11   \n",
       "12  184632.406250  8.299420e+10  8.165568e+10  217551.875000  8.165568e+10   \n",
       "29  243918.390625  1.481313e+11  1.106875e+11  232091.109375  1.106875e+11   \n",
       "17  238575.859375  1.416299e+11  1.030976e+11  234851.562500  1.030976e+11   \n",
       "31  253775.859375  1.560586e+11  1.176267e+11  241852.796875  1.176267e+11   \n",
       "19  389014.781250  2.721428e+11  1.317893e+11  257560.718750  1.317893e+11   \n",
       "27  519540.218750  4.158681e+11  3.807378e+11  521109.156250  3.807378e+11   \n",
       "18  542045.187500  4.358172e+11  3.865410e+11  527837.625000  3.865410e+11   \n",
       "22  542052.875000  4.358255e+11  3.865486e+11  527844.875000  3.865486e+11   \n",
       "30  542053.500000  4.358261e+11  3.865487e+11  527844.875000  3.865487e+11   \n",
       "26  542105.500000  4.358985e+11  3.866162e+11  527898.312500  3.866162e+11   \n",
       "16  542361.250000  4.361600e+11  3.868919e+11  528169.875000  3.868919e+11   \n",
       "28  542362.812500  4.361619e+11  3.868937e+11  528171.687500  3.868937e+11   \n",
       "20  542363.562500  4.361628e+11  3.868939e+11  528171.875000  3.868939e+11   \n",
       "24  542365.437500  4.361642e+11  3.868946e+11  528172.500000  3.868946e+11   \n",
       "1             NaN           NaN           NaN            NaN           NaN   \n",
       "3             NaN           NaN           NaN            NaN           NaN   \n",
       "5             NaN           NaN           NaN            NaN           NaN   \n",
       "7             NaN           NaN           NaN            NaN           NaN   \n",
       "9             NaN           NaN           NaN            NaN           NaN   \n",
       "11            NaN           NaN           NaN            NaN           NaN   \n",
       "13            NaN           NaN           NaN            NaN           NaN   \n",
       "15            NaN           NaN           NaN            NaN           NaN   \n",
       "\n",
       "   activation  dense_layers  dropout  nodes optimizer  \n",
       "2        relu             5      0.2    200      adam  \n",
       "0        relu             5      0.2    100      adam  \n",
       "8        relu            10      0.2    100      adam  \n",
       "6        relu             5      0.5    200      adam  \n",
       "4        relu             5      0.5    100      adam  \n",
       "10       relu            10      0.2    200      adam  \n",
       "21       tanh             5      0.5    100       sgd  \n",
       "14       relu            10      0.5    200      adam  \n",
       "23       tanh             5      0.5    200       sgd  \n",
       "25       tanh            10      0.2    100       sgd  \n",
       "12       relu            10      0.5    100      adam  \n",
       "29       tanh            10      0.5    100       sgd  \n",
       "17       tanh             5      0.2    100       sgd  \n",
       "31       tanh            10      0.5    200       sgd  \n",
       "19       tanh             5      0.2    200       sgd  \n",
       "27       tanh            10      0.2    200       sgd  \n",
       "18       tanh             5      0.2    200      adam  \n",
       "22       tanh             5      0.5    200      adam  \n",
       "30       tanh            10      0.5    200      adam  \n",
       "26       tanh            10      0.2    200      adam  \n",
       "16       tanh             5      0.2    100      adam  \n",
       "28       tanh            10      0.5    100      adam  \n",
       "20       tanh             5      0.5    100      adam  \n",
       "24       tanh            10      0.2    100      adam  \n",
       "1        relu             5      0.2    100       sgd  \n",
       "3        relu             5      0.2    200       sgd  \n",
       "5        relu             5      0.5    100       sgd  \n",
       "7        relu             5      0.5    200       sgd  \n",
       "9        relu            10      0.2    100       sgd  \n",
       "11       relu            10      0.2    200       sgd  \n",
       "13       relu            10      0.5    100       sgd  \n",
       "15       relu            10      0.5    200       sgd  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.data.sort_values(by='val_mae', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the top `n_models` tested by the scan object and do cross validation on a given dataset for further validation using the `.evaluate_models` method.  This adds columns to the `.data` dataframe attribute with the mean scores of cross validation.  Notice we have to define the metric we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Added evaluation score columns to scan_object.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results.evaluate_models(X_val.values,\n",
    "                        y_val.values,\n",
    "                        task='continuous',\n",
    "                        n_models = 10,\n",
    "                        metric='val_mae',\n",
    "                        folds=5,\n",
    "                        shuffle=True,\n",
    "                        asc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>activation</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>nodes</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>eval_mae_mean</th>\n",
       "      <th>eval_mae_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/03/21-152448</td>\n",
       "      <td>08/03/21-152458</td>\n",
       "      <td>9.823584</td>\n",
       "      <td>10</td>\n",
       "      <td>2.144835e+10</td>\n",
       "      <td>90865.343750</td>\n",
       "      <td>2.144835e+10</td>\n",
       "      <td>1.389579e+10</td>\n",
       "      <td>75770.218750</td>\n",
       "      <td>1.389579e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>75770.221036</td>\n",
       "      <td>1574.864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/03/21-152439</td>\n",
       "      <td>08/03/21-152445</td>\n",
       "      <td>6.439806</td>\n",
       "      <td>10</td>\n",
       "      <td>2.677377e+10</td>\n",
       "      <td>100079.570312</td>\n",
       "      <td>2.677377e+10</td>\n",
       "      <td>1.526509e+10</td>\n",
       "      <td>78941.773438</td>\n",
       "      <td>1.526509e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>78941.760156</td>\n",
       "      <td>4474.300579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08/03/21-152520</td>\n",
       "      <td>08/03/21-152529</td>\n",
       "      <td>9.569853</td>\n",
       "      <td>10</td>\n",
       "      <td>3.657275e+10</td>\n",
       "      <td>120794.320312</td>\n",
       "      <td>3.657274e+10</td>\n",
       "      <td>1.645498e+10</td>\n",
       "      <td>80395.109375</td>\n",
       "      <td>1.645498e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>80395.106453</td>\n",
       "      <td>1927.723393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08/03/21-152509</td>\n",
       "      <td>08/03/21-152517</td>\n",
       "      <td>8.088457</td>\n",
       "      <td>8</td>\n",
       "      <td>3.464387e+10</td>\n",
       "      <td>117413.835938</td>\n",
       "      <td>3.464387e+10</td>\n",
       "      <td>1.621678e+10</td>\n",
       "      <td>82495.007812</td>\n",
       "      <td>1.621678e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>82495.003102</td>\n",
       "      <td>4792.702896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/03/21-152501</td>\n",
       "      <td>08/03/21-152507</td>\n",
       "      <td>6.242362</td>\n",
       "      <td>10</td>\n",
       "      <td>4.506420e+10</td>\n",
       "      <td>135867.921875</td>\n",
       "      <td>4.506420e+10</td>\n",
       "      <td>1.726049e+10</td>\n",
       "      <td>83724.015625</td>\n",
       "      <td>1.726049e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>83724.017558</td>\n",
       "      <td>2003.016808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>08/03/21-152532</td>\n",
       "      <td>08/03/21-152547</td>\n",
       "      <td>15.117230</td>\n",
       "      <td>9</td>\n",
       "      <td>2.975040e+10</td>\n",
       "      <td>108701.460938</td>\n",
       "      <td>2.975039e+10</td>\n",
       "      <td>2.763869e+10</td>\n",
       "      <td>125190.648438</td>\n",
       "      <td>2.763868e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>125190.657219</td>\n",
       "      <td>1902.396582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>08/03/21-152643</td>\n",
       "      <td>08/03/21-152646</td>\n",
       "      <td>3.114382</td>\n",
       "      <td>4</td>\n",
       "      <td>1.428123e+11</td>\n",
       "      <td>239281.156250</td>\n",
       "      <td>1.428123e+11</td>\n",
       "      <td>8.732812e+10</td>\n",
       "      <td>189113.937500</td>\n",
       "      <td>8.732812e+10</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>189113.936389</td>\n",
       "      <td>11389.005095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>08/03/21-152600</td>\n",
       "      <td>08/03/21-152609</td>\n",
       "      <td>9.152887</td>\n",
       "      <td>5</td>\n",
       "      <td>6.431127e+10</td>\n",
       "      <td>163381.578125</td>\n",
       "      <td>6.431127e+10</td>\n",
       "      <td>6.110067e+10</td>\n",
       "      <td>195479.671875</td>\n",
       "      <td>6.110067e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>195479.650556</td>\n",
       "      <td>3487.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>08/03/21-152656</td>\n",
       "      <td>08/03/21-152704</td>\n",
       "      <td>8.040211</td>\n",
       "      <td>9</td>\n",
       "      <td>1.495984e+11</td>\n",
       "      <td>247378.328125</td>\n",
       "      <td>1.495984e+11</td>\n",
       "      <td>1.035998e+11</td>\n",
       "      <td>208676.718750</td>\n",
       "      <td>1.035998e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>208671.475521</td>\n",
       "      <td>5976.994117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>08/03/21-152715</td>\n",
       "      <td>08/03/21-152720</td>\n",
       "      <td>5.534511</td>\n",
       "      <td>7</td>\n",
       "      <td>1.511078e+11</td>\n",
       "      <td>246557.171875</td>\n",
       "      <td>1.511078e+11</td>\n",
       "      <td>1.118859e+11</td>\n",
       "      <td>212283.421875</td>\n",
       "      <td>1.118859e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>212276.861817</td>\n",
       "      <td>10904.043196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/03/21-152446</td>\n",
       "      <td>08/03/21-152448</td>\n",
       "      <td>1.890808</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/03/21-152458</td>\n",
       "      <td>08/03/21-152501</td>\n",
       "      <td>2.411776</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08/03/21-152507</td>\n",
       "      <td>08/03/21-152509</td>\n",
       "      <td>1.559266</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08/03/21-152517</td>\n",
       "      <td>08/03/21-152520</td>\n",
       "      <td>2.318742</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>08/03/21-152530</td>\n",
       "      <td>08/03/21-152532</td>\n",
       "      <td>2.324288</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08/03/21-152548</td>\n",
       "      <td>08/03/21-152551</td>\n",
       "      <td>3.728953</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>08/03/21-152552</td>\n",
       "      <td>08/03/21-152557</td>\n",
       "      <td>5.529936</td>\n",
       "      <td>6</td>\n",
       "      <td>8.299420e+10</td>\n",
       "      <td>184632.406250</td>\n",
       "      <td>8.299420e+10</td>\n",
       "      <td>8.165568e+10</td>\n",
       "      <td>217551.875000</td>\n",
       "      <td>8.165568e+10</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>08/03/21-152557</td>\n",
       "      <td>08/03/21-152600</td>\n",
       "      <td>2.183546</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>08/03/21-152609</td>\n",
       "      <td>08/03/21-152613</td>\n",
       "      <td>4.137468</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>08/03/21-152614</td>\n",
       "      <td>08/03/21-152619</td>\n",
       "      <td>5.930509</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361600e+11</td>\n",
       "      <td>542361.250000</td>\n",
       "      <td>4.361600e+11</td>\n",
       "      <td>3.868919e+11</td>\n",
       "      <td>528169.875000</td>\n",
       "      <td>3.868919e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>08/03/21-152620</td>\n",
       "      <td>08/03/21-152623</td>\n",
       "      <td>3.120467</td>\n",
       "      <td>6</td>\n",
       "      <td>1.416299e+11</td>\n",
       "      <td>238575.859375</td>\n",
       "      <td>1.416299e+11</td>\n",
       "      <td>1.030976e+11</td>\n",
       "      <td>234851.562500</td>\n",
       "      <td>1.030976e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>08/03/21-152623</td>\n",
       "      <td>08/03/21-152633</td>\n",
       "      <td>9.954805</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358172e+11</td>\n",
       "      <td>542045.187500</td>\n",
       "      <td>4.358172e+11</td>\n",
       "      <td>3.865410e+11</td>\n",
       "      <td>527837.625000</td>\n",
       "      <td>3.865410e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>08/03/21-152633</td>\n",
       "      <td>08/03/21-152637</td>\n",
       "      <td>3.320946</td>\n",
       "      <td>3</td>\n",
       "      <td>2.721428e+11</td>\n",
       "      <td>389014.781250</td>\n",
       "      <td>2.721428e+11</td>\n",
       "      <td>1.317893e+11</td>\n",
       "      <td>257560.718750</td>\n",
       "      <td>1.317893e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>08/03/21-152637</td>\n",
       "      <td>08/03/21-152643</td>\n",
       "      <td>6.008372</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361628e+11</td>\n",
       "      <td>542363.562500</td>\n",
       "      <td>4.361628e+11</td>\n",
       "      <td>3.868939e+11</td>\n",
       "      <td>528171.875000</td>\n",
       "      <td>3.868939e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>08/03/21-152646</td>\n",
       "      <td>08/03/21-152656</td>\n",
       "      <td>9.653022</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358254e+11</td>\n",
       "      <td>542052.875000</td>\n",
       "      <td>4.358255e+11</td>\n",
       "      <td>3.865486e+11</td>\n",
       "      <td>527844.875000</td>\n",
       "      <td>3.865486e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>08/03/21-152705</td>\n",
       "      <td>08/03/21-152714</td>\n",
       "      <td>9.828249</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361642e+11</td>\n",
       "      <td>542365.437500</td>\n",
       "      <td>4.361642e+11</td>\n",
       "      <td>3.868946e+11</td>\n",
       "      <td>528172.500000</td>\n",
       "      <td>3.868946e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>08/03/21-152720</td>\n",
       "      <td>08/03/21-152739</td>\n",
       "      <td>18.373596</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358985e+11</td>\n",
       "      <td>542105.500000</td>\n",
       "      <td>4.358985e+11</td>\n",
       "      <td>3.866162e+11</td>\n",
       "      <td>527898.312500</td>\n",
       "      <td>3.866162e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>08/03/21-152739</td>\n",
       "      <td>08/03/21-152744</td>\n",
       "      <td>5.378550</td>\n",
       "      <td>3</td>\n",
       "      <td>4.158681e+11</td>\n",
       "      <td>519540.218750</td>\n",
       "      <td>4.158681e+11</td>\n",
       "      <td>3.807378e+11</td>\n",
       "      <td>521109.156250</td>\n",
       "      <td>3.807378e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>08/03/21-152745</td>\n",
       "      <td>08/03/21-152753</td>\n",
       "      <td>8.598542</td>\n",
       "      <td>10</td>\n",
       "      <td>4.361619e+11</td>\n",
       "      <td>542362.812500</td>\n",
       "      <td>4.361619e+11</td>\n",
       "      <td>3.868937e+11</td>\n",
       "      <td>528171.687500</td>\n",
       "      <td>3.868937e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>08/03/21-152753</td>\n",
       "      <td>08/03/21-152758</td>\n",
       "      <td>4.642022</td>\n",
       "      <td>6</td>\n",
       "      <td>1.481313e+11</td>\n",
       "      <td>243918.390625</td>\n",
       "      <td>1.481313e+11</td>\n",
       "      <td>1.106875e+11</td>\n",
       "      <td>232091.109375</td>\n",
       "      <td>1.106875e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>08/03/21-152758</td>\n",
       "      <td>08/03/21-152816</td>\n",
       "      <td>17.764553</td>\n",
       "      <td>10</td>\n",
       "      <td>4.358260e+11</td>\n",
       "      <td>542053.500000</td>\n",
       "      <td>4.358261e+11</td>\n",
       "      <td>3.865487e+11</td>\n",
       "      <td>527844.875000</td>\n",
       "      <td>3.865487e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>08/03/21-152816</td>\n",
       "      <td>08/03/21-152822</td>\n",
       "      <td>5.356652</td>\n",
       "      <td>3</td>\n",
       "      <td>1.560586e+11</td>\n",
       "      <td>253775.859375</td>\n",
       "      <td>1.560586e+11</td>\n",
       "      <td>1.176267e+11</td>\n",
       "      <td>241852.796875</td>\n",
       "      <td>1.176267e+11</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              start              end   duration  round_epochs          loss  \\\n",
       "2   08/03/21-152448  08/03/21-152458   9.823584            10  2.144835e+10   \n",
       "0   08/03/21-152439  08/03/21-152445   6.439806            10  2.677377e+10   \n",
       "8   08/03/21-152520  08/03/21-152529   9.569853            10  3.657275e+10   \n",
       "6   08/03/21-152509  08/03/21-152517   8.088457             8  3.464387e+10   \n",
       "4   08/03/21-152501  08/03/21-152507   6.242362            10  4.506420e+10   \n",
       "10  08/03/21-152532  08/03/21-152547  15.117230             9  2.975040e+10   \n",
       "21  08/03/21-152643  08/03/21-152646   3.114382             4  1.428123e+11   \n",
       "14  08/03/21-152600  08/03/21-152609   9.152887             5  6.431127e+10   \n",
       "23  08/03/21-152656  08/03/21-152704   8.040211             9  1.495984e+11   \n",
       "25  08/03/21-152715  08/03/21-152720   5.534511             7  1.511078e+11   \n",
       "1   08/03/21-152446  08/03/21-152448   1.890808             2           NaN   \n",
       "3   08/03/21-152458  08/03/21-152501   2.411776             2           NaN   \n",
       "5   08/03/21-152507  08/03/21-152509   1.559266             2           NaN   \n",
       "7   08/03/21-152517  08/03/21-152520   2.318742             2           NaN   \n",
       "9   08/03/21-152530  08/03/21-152532   2.324288             2           NaN   \n",
       "11  08/03/21-152548  08/03/21-152551   3.728953             2           NaN   \n",
       "12  08/03/21-152552  08/03/21-152557   5.529936             6  8.299420e+10   \n",
       "13  08/03/21-152557  08/03/21-152600   2.183546             2           NaN   \n",
       "15  08/03/21-152609  08/03/21-152613   4.137468             2           NaN   \n",
       "16  08/03/21-152614  08/03/21-152619   5.930509            10  4.361600e+11   \n",
       "17  08/03/21-152620  08/03/21-152623   3.120467             6  1.416299e+11   \n",
       "18  08/03/21-152623  08/03/21-152633   9.954805            10  4.358172e+11   \n",
       "19  08/03/21-152633  08/03/21-152637   3.320946             3  2.721428e+11   \n",
       "20  08/03/21-152637  08/03/21-152643   6.008372            10  4.361628e+11   \n",
       "22  08/03/21-152646  08/03/21-152656   9.653022            10  4.358254e+11   \n",
       "24  08/03/21-152705  08/03/21-152714   9.828249            10  4.361642e+11   \n",
       "26  08/03/21-152720  08/03/21-152739  18.373596            10  4.358985e+11   \n",
       "27  08/03/21-152739  08/03/21-152744   5.378550             3  4.158681e+11   \n",
       "28  08/03/21-152745  08/03/21-152753   8.598542            10  4.361619e+11   \n",
       "29  08/03/21-152753  08/03/21-152758   4.642022             6  1.481313e+11   \n",
       "30  08/03/21-152758  08/03/21-152816  17.764553            10  4.358260e+11   \n",
       "31  08/03/21-152816  08/03/21-152822   5.356652             3  1.560586e+11   \n",
       "\n",
       "              mae           mse      val_loss        val_mae       val_mse  \\\n",
       "2    90865.343750  2.144835e+10  1.389579e+10   75770.218750  1.389579e+10   \n",
       "0   100079.570312  2.677377e+10  1.526509e+10   78941.773438  1.526509e+10   \n",
       "8   120794.320312  3.657274e+10  1.645498e+10   80395.109375  1.645498e+10   \n",
       "6   117413.835938  3.464387e+10  1.621678e+10   82495.007812  1.621678e+10   \n",
       "4   135867.921875  4.506420e+10  1.726049e+10   83724.015625  1.726049e+10   \n",
       "10  108701.460938  2.975039e+10  2.763869e+10  125190.648438  2.763868e+10   \n",
       "21  239281.156250  1.428123e+11  8.732812e+10  189113.937500  8.732812e+10   \n",
       "14  163381.578125  6.431127e+10  6.110067e+10  195479.671875  6.110067e+10   \n",
       "23  247378.328125  1.495984e+11  1.035998e+11  208676.718750  1.035998e+11   \n",
       "25  246557.171875  1.511078e+11  1.118859e+11  212283.421875  1.118859e+11   \n",
       "1             NaN           NaN           NaN            NaN           NaN   \n",
       "3             NaN           NaN           NaN            NaN           NaN   \n",
       "5             NaN           NaN           NaN            NaN           NaN   \n",
       "7             NaN           NaN           NaN            NaN           NaN   \n",
       "9             NaN           NaN           NaN            NaN           NaN   \n",
       "11            NaN           NaN           NaN            NaN           NaN   \n",
       "12  184632.406250  8.299420e+10  8.165568e+10  217551.875000  8.165568e+10   \n",
       "13            NaN           NaN           NaN            NaN           NaN   \n",
       "15            NaN           NaN           NaN            NaN           NaN   \n",
       "16  542361.250000  4.361600e+11  3.868919e+11  528169.875000  3.868919e+11   \n",
       "17  238575.859375  1.416299e+11  1.030976e+11  234851.562500  1.030976e+11   \n",
       "18  542045.187500  4.358172e+11  3.865410e+11  527837.625000  3.865410e+11   \n",
       "19  389014.781250  2.721428e+11  1.317893e+11  257560.718750  1.317893e+11   \n",
       "20  542363.562500  4.361628e+11  3.868939e+11  528171.875000  3.868939e+11   \n",
       "22  542052.875000  4.358255e+11  3.865486e+11  527844.875000  3.865486e+11   \n",
       "24  542365.437500  4.361642e+11  3.868946e+11  528172.500000  3.868946e+11   \n",
       "26  542105.500000  4.358985e+11  3.866162e+11  527898.312500  3.866162e+11   \n",
       "27  519540.218750  4.158681e+11  3.807378e+11  521109.156250  3.807378e+11   \n",
       "28  542362.812500  4.361619e+11  3.868937e+11  528171.687500  3.868937e+11   \n",
       "29  243918.390625  1.481313e+11  1.106875e+11  232091.109375  1.106875e+11   \n",
       "30  542053.500000  4.358261e+11  3.865487e+11  527844.875000  3.865487e+11   \n",
       "31  253775.859375  1.560586e+11  1.176267e+11  241852.796875  1.176267e+11   \n",
       "\n",
       "   activation  dense_layers  dropout  nodes optimizer  eval_mae_mean  \\\n",
       "2        relu             5      0.2    200      adam   75770.221036   \n",
       "0        relu             5      0.2    100      adam   78941.760156   \n",
       "8        relu            10      0.2    100      adam   80395.106453   \n",
       "6        relu             5      0.5    200      adam   82495.003102   \n",
       "4        relu             5      0.5    100      adam   83724.017558   \n",
       "10       relu            10      0.2    200      adam  125190.657219   \n",
       "21       tanh             5      0.5    100       sgd  189113.936389   \n",
       "14       relu            10      0.5    200      adam  195479.650556   \n",
       "23       tanh             5      0.5    200       sgd  208671.475521   \n",
       "25       tanh            10      0.2    100       sgd  212276.861817   \n",
       "1        relu             5      0.2    100       sgd            NaN   \n",
       "3        relu             5      0.2    200       sgd            NaN   \n",
       "5        relu             5      0.5    100       sgd            NaN   \n",
       "7        relu             5      0.5    200       sgd            NaN   \n",
       "9        relu            10      0.2    100       sgd            NaN   \n",
       "11       relu            10      0.2    200       sgd            NaN   \n",
       "12       relu            10      0.5    100      adam            NaN   \n",
       "13       relu            10      0.5    100       sgd            NaN   \n",
       "15       relu            10      0.5    200       sgd            NaN   \n",
       "16       tanh             5      0.2    100      adam            NaN   \n",
       "17       tanh             5      0.2    100       sgd            NaN   \n",
       "18       tanh             5      0.2    200      adam            NaN   \n",
       "19       tanh             5      0.2    200       sgd            NaN   \n",
       "20       tanh             5      0.5    100      adam            NaN   \n",
       "22       tanh             5      0.5    200      adam            NaN   \n",
       "24       tanh            10      0.2    100      adam            NaN   \n",
       "26       tanh            10      0.2    200      adam            NaN   \n",
       "27       tanh            10      0.2    200       sgd            NaN   \n",
       "28       tanh            10      0.5    100      adam            NaN   \n",
       "29       tanh            10      0.5    100       sgd            NaN   \n",
       "30       tanh            10      0.5    200      adam            NaN   \n",
       "31       tanh            10      0.5    200       sgd            NaN   \n",
       "\n",
       "    eval_mae_std  \n",
       "2    1574.864746  \n",
       "0    4474.300579  \n",
       "8    1927.723393  \n",
       "6    4792.702896  \n",
       "4    2003.016808  \n",
       "10   1902.396582  \n",
       "21  11389.005095  \n",
       "14   3487.561224  \n",
       "23   5976.994117  \n",
       "25  10904.043196  \n",
       "1            NaN  \n",
       "3            NaN  \n",
       "5            NaN  \n",
       "7            NaN  \n",
       "9            NaN  \n",
       "11           NaN  \n",
       "12           NaN  \n",
       "13           NaN  \n",
       "15           NaN  \n",
       "16           NaN  \n",
       "17           NaN  \n",
       "18           NaN  \n",
       "19           NaN  \n",
       "20           NaN  \n",
       "22           NaN  \n",
       "24           NaN  \n",
       "26           NaN  \n",
       "27           NaN  \n",
       "28           NaN  \n",
       "29           NaN  \n",
       "30           NaN  \n",
       "31           NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.data.sort_values(by='eval_mae_mean', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results.best_model(metric='mse', asc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               17600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 178,601\n",
      "Trainable params: 178,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveling Up\n",
    "\n",
    "A full Gridsearch can take a long time, especially with deep learning models that tend to be slower to train than traditional models.  One way to reduce the search time is to use a reduction parameter in your Talos Scan object initialization.  This argument will use previous results to remove future experiments that are unlikely to return improved results.  This is called *Probablistic Reduction*. \n",
    "\n",
    "You can learn more at: https://autonomio.github.io/talos/#/Probabilistic_Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model.h5')\n",
    "best_model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "final model score on holdout: 79674.75\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('best_model.h5')\n",
    "y_pred = my_model.predict(X_holdout)\n",
    "score = mean_absolute_error(y_holdout, y_pred)\n",
    "print(f'final model score on holdout: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice:  \n",
    "`results.best_model()` returns and uncompiled model.  While we can use the model for prediction, we won't be able to train it further until we recompile it with the desired optimizer and loss function.  These can be referenced in the experimental record from the results file.  \n",
    "\n",
    "Recompiling the model will reset the weights, but they can then be reloaded from the saved weights file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Talos walkthrough: https://medium.com/swlh/how-to-perform-keras-hyperparameter-optimization-x3-faster-on-tpu-for-free-602b97812602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer Learning and Pretrained Models\n",
    "\n",
    "* A pretrained network (also known in image classification as a convolutional base) consists of layers that have already been trained on typically general data\n",
    "* For images, these layers have already learned general patterns, textures, colors, etc. such that when you feed in your training data, certain features can immediately be detected. This part is **feature extraction**.\n",
    "* You typically add your own final layers to train the network to classify/regress based on your problem. This component is **fine tuning**\n",
    "\n",
    "Here are the pretrained models that exist within Keras: https://keras.io/api/applications/\n",
    "\n",
    "To demonstrate the utility of pretrained networks, we'll compare model performance between a baseline model and a model using a pretrained network (VGG19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "We will be shifting gears now and moving to some transfer learning using image classificaiton.  We will be using the Cifar10 dataset, a dataset of 60,000 images divided into 10 categories for our model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_img, y_train_img), (X_test_img, y_test_img) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train_img = keras.utils.to_categorical(y_train_img)\n",
    "y_test_img = keras.utils.to_categorical(y_test_img)\n",
    "\n",
    "image_size = (X_train_img.shape[1], \n",
    "              X_train_img.shape[2], \n",
    "              X_train_img.shape[3])\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "for i in range(2):\n",
    "    axes[i].imshow(X_train_img[i])\n",
    "    axes[i].set_title(y_train_img[i])\n",
    "    \n",
    "fig.suptitle('First Two Images in Cifar10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Pretrained Layers\n",
    "\n",
    "\n",
    "## VGG1\n",
    "\n",
    "[VGG16](https://neurohive.io/en/popular-networks/vgg16/) is a very deep image classification model architecture.  \n",
    "\n",
    "It starts with a set input shape, (224,224, 3).  This represents a 2 dimensional RGB image (first two dimensions) and 3 color channels (last dimension).\n",
    "\n",
    "It uses convolutional and max pooling layers, but don't worry about what these are yet.  We will learn more about these soon.\n",
    "\n",
    "We will be setting the weights of this model to a collection of pre-trained weights called 'imagenet'.  These weights are the result of the VGG16 model's training on the [imagenet dataset](https://en.wikipedia.org/wiki/ImageNet).  This is a database of 14 million images in 20,000 different categories.  \n",
    "\n",
    "VGG19: https://keras.io/api/applications/vgg/#vgg19-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load the VGG16 model\n",
    "from keras.applications import VGG16\n",
    "pretrained = VGG16(weights='imagenet', \n",
    "                 include_top=False, \n",
    "                 input_shape=image_size)\n",
    "\n",
    "pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_transfer = Sequential()\n",
    "cnn_transfer.add(pretrained)\n",
    "\n",
    "# freezing layers so they don't get retrained with your new data\n",
    "for layer in cnn_transfer.layers:\n",
    "    layer.trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our own dense layers\n",
    "cnn_transfer.add(layers.Flatten())\n",
    "cnn_transfer.add(layers.Dense(132, activation='relu'))\n",
    "cnn_transfer.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 False\n",
      "flatten True\n",
      "dense True\n",
      "dense_1 True\n"
     ]
    }
   ],
   "source": [
    "# to verify that the weights are \"frozen\" \n",
    "for layer in cnn_transfer.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 132)               67716     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1330      \n",
      "=================================================================\n",
      "Total params: 14,783,734\n",
      "Trainable params: 69,046\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_transfer.compile(loss='categorical_crossentropy', \n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "cnn_transfer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        16448     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          16448     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 132)               135300    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1330      \n",
      "=================================================================\n",
      "Total params: 252,598\n",
      "Trainable params: 252,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_custom = Sequential()\n",
    "cnn_custom.add(layers.Input(shape=image_size))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Flatten())\n",
    "cnn_custom.add(Dense(132, activation='relu'))\n",
    "cnn_custom.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_custom.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "cnn_custom.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test each:\n",
    "\n",
    "Notice that the custom model is much shallower than VGG16, but has more trainable weights.  Take note of accuracy and training time between the two models.\n",
    "\n",
    "Transfer learning is not always the most successful tactic, but it tends to reduct training time and is sometimes more successful.  It is one more tool for the toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 98s 2s/step - loss: 2.4927 - accuracy: 0.1654 - val_loss: 1.9673 - val_accuracy: 0.2696\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.9713 - accuracy: 0.2650\n",
      "Loss of custom model on test set: 1.97\n",
      "Accuracy of custom model on test set:  0.26\n",
      "CPU times: user 9min 22s, sys: 2min 18s, total: 11min 40s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit and evaluate the custom CNN.\n",
    "cnn_custom.fit(X_train_img, y_train_img,\n",
    "               epochs=1,\n",
    "               batch_size=1024,\n",
    "               validation_split=.2)\n",
    "custom_loss, custom_accuracy = cnn_custom.evaluate(X_test_img, y_test_img)\n",
    "\n",
    "print(f'Loss of custom model on test set: {custom_loss:.2f}')\n",
    "print(f'Accuracy of custom model on test set: {custom_accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 174s 4s/step - loss: 11.8640 - accuracy: 0.2586 - val_loss: 3.7308 - val_accuracy: 0.4276\n",
      "313/313 [==============================] - 51s 164ms/step - loss: 3.7566 - accuracy: 0.4335\n",
      "Loss of transfer model on test set: 3.76\n",
      "Accuracy of transfer model on test set:  0.43\n",
      "CPU times: user 25min 30s, sys: 2min 12s, total: 27min 43s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit and evaluate the CNN using transfer learning.\n",
    "cnn_transfer.fit(X_train_img, y_train_img,\n",
    "                 epochs=1,\n",
    "                 batch_size=1024,\n",
    "                 validation_split=.2)\n",
    "transfer_loss, transfer_accuracy = cnn_transfer.evaluate(X_test_img, y_test_img)\n",
    "print(f'Loss of transfer model on test set: {transfer_loss:.2f}')\n",
    "print(f'Accuracy of transfer model on test set: {transfer_accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "1. Used Talos to search for optimal hyperparameters for a multi-layered perceptron with dense and dropout layers.\n",
    "2. Saved the tuned model and reloaded it using saved weights.\n",
    "3. Used transfer learning to reduce the training time for an image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.173px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
