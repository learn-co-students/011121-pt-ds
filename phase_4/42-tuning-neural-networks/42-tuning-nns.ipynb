{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 42: Transfer Learning and Tuning Neural Networks\n",
    "\n",
    "1. Using GridSearch/Talos for finding optimal parameter combinations \n",
    "2. Saving your neural network to disk\n",
    "4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tuning hyperparameters with Keras Models\n",
    "\n",
    "There are a couple ways to go about testing combinations of parameters, GridSearch style:\n",
    "* **Using SKlearn GridSearch**: https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/\n",
    "    * This involves creating a model object such that scikit-learn's existing GridSearch functions work with your neural net.\n",
    "* **Using KerasTuner**: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "* **Using Talos**: https://autonomio.github.io/talos/#/Scan\n",
    "    * This library lets you tune without having to create the model object, and also can automatically output your parameter combination scores into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install talos\n",
    "import talos\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "Let's return to our Seattle Housing data from the last study group.\n",
    "\n",
    "We are going to split the testing set into a validation and a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/011121-pt-ds/main/phase_4/41-building-deep-neural-networks/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/011121-pt-ds/main/phase_4/41-building-deep-neural-networks/test.csv')\n",
    "split = int(len(test)*.5)\n",
    "val = test.iloc[:split, :]\n",
    "holdout = test.iloc[split:, :]\n",
    "\n",
    "X_train, y_train = (train.drop('price', axis=1), train['price'])\n",
    "X_val, y_val = (val.drop('price', axis=1), val['price'])\n",
    "X_holdout, y_holdout = (holdout.drop('price', axis=1), holdout['price'])\n",
    "\n",
    "display(train)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Function to Create a Model\n",
    "\n",
    "To work with Talos, we create a specially formed function that returns our model and the model training history.\n",
    "\n",
    "The function must take: `X_train, y_train, X_test, y_test, params` in that order.  Params is a dictionary of parameters.  Keys should be a name for the hyperparameter.  It is arbitrary, but should be something descriptive.  The values are the range of values that hyperparameter could take.\n",
    "\n",
    "Talos will pass only one of the possible values for each hyperparameter to the function on each experiment in the `params` dictionary that the function expects.\n",
    "\n",
    "We then set the hyperparameters of the model generated in the function to be the value we want to try from the params dictionary.\n",
    "\n",
    "Another note: we can adjust the depth of the model by adding layers in a loop, a series of loops, or even nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_network(x_train, y_train, x_test, y_test, params):\n",
    "\n",
    "    #we build the model like we would normally do it\n",
    "    model = Sequential()\n",
    "    \n",
    "    #input layer\n",
    "    model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # hidden layers\n",
    "    for layer in range(params['dense_layers']):\n",
    "        model.add(layers.Dense(params['nodes'], activation=params['activation']))\n",
    "        model.add(layers.Dropout(params['dropout']))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=params['optimizer'], metrics=['mae', 'mse'])\n",
    "    \n",
    "    #callback to prevent over-training\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    out = model.fit(x_train, y_train, \n",
    "                   validation_data=(x_test, y_test),\n",
    "                   batch_size=50,\n",
    "                   epochs=10,\n",
    "                   verbose=0,\n",
    "                   callbacks = [earlystopping])\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:\n",
    "\n",
    "Define a dictionary of possible parameter values.  Remember that more parameters quickly multiply the number of models Talos will compare.  It's okay to do a series of experiments based on the results of your previous ones, rather than on big gridsearch.\n",
    "\n",
    "Alternatively you can use [Probabalistic Reduction](https://autonomio.github.io/talos/#/Probabilistic_Reduction) to instruct Scan to keep hyperparameter values that have shown to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dropout': [0.2, 0.5], \n",
    "          'optimizer': ['adam', 'sgd'], \n",
    "          'activation': ['relu', 'tanh'], \n",
    "          'dense_layers': [5,10],\n",
    "          'nodes': [100,200]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`talos.Scan()` instantiates a new scan object by conducting all of the experiments defined in `params` using the model defined in the `model=` argument and the data passed.\n",
    "\n",
    "your scan object (named `results` below) will contain the record of your experiments, including all of the fitted models.  It also saves records to disk under the folder defined in `experiment_name=`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = talos.Scan(X_train, y_train, \n",
    "                     x_val=X_val,\n",
    "                     y_val=y_val,\n",
    "                     params=params, \n",
    "                     model=dense_network,\n",
    "                     experiment_name='grid',\n",
    "                     minimize_loss=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Record\n",
    "\n",
    "The record of all experiments is stored in a dataframe in the `.data` attribute.  \n",
    "\n",
    "The scan object will keep a record of scores according to the metrics defined when the model was compiled.  You can use these metrics to order the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.data.sort_values(by='val_mae', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the top `n_models` tested by the scan object and do cross validation on a given dataset for further validation using the `.evaluate_models` method.  This adds columns to the `.data` dataframe attribute with the mean scores of cross validation.  Notice we have to define the metric we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.evaluate_models(X_val.values,\n",
    "                        y_val.values,\n",
    "                        task='continuous',\n",
    "                        n_models = 10,\n",
    "                        metric='val_mae',\n",
    "                        folds=5,\n",
    "                        shuffle=True,\n",
    "                        asc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.data.sort_values(by='eval_mae_mean', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results.best_model(metric='mse', asc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveling Up\n",
    "\n",
    "A full Gridsearch can take a long time, especially with deep learning models that tend to be slower to train than traditional models.  One way to reduce the search time is to use a reduction parameter in your Talos Scan object initialization.  This argument will use previous results to remove future experiments that are unlikely to return improved results.  This is called *Probablistic Reduction*. \n",
    "\n",
    "You can learn more at: https://autonomio.github.io/talos/#/Probabilistic_Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model.h5')\n",
    "best_model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('best_model.h5')\n",
    "y_pred = my_model.predict(X_holdout)\n",
    "score = mean_absolute_error(y_holdout, y_pred)\n",
    "print(f'final model score on holdout: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice:  \n",
    "`results.best_model()` returns and uncompiled model.  While we can use the model for prediction, we won't be able to train it further until we recompile it with the desired optimizer and loss function.  These can be referenced in the experimental record from the results file.  \n",
    "\n",
    "Recompiling the model will reset the weights, but they can then be reloaded from the saved weights file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Talos walkthrough: https://medium.com/swlh/how-to-perform-keras-hyperparameter-optimization-x3-faster-on-tpu-for-free-602b97812602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer Learning and Pretrained Models\n",
    "\n",
    "* A pretrained network (also known in image classification as a convolutional base) consists of layers that have already been trained on typically general data\n",
    "* For images, these layers have already learned general patterns, textures, colors, etc. such that when you feed in your training data, certain features can immediately be detected. This part is **feature extraction**.\n",
    "* You typically add your own final layers to train the network to classify/regress based on your problem. This component is **fine tuning**\n",
    "\n",
    "Here are the pretrained models that exist within Keras: https://keras.io/api/applications/\n",
    "\n",
    "To demonstrate the utility of pretrained networks, we'll compare model performance between a baseline model and a model using a pretrained network (VGG19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "We will be shifting gears now and moving to some transfer learning using image classificaiton.  We will be using the Cifar10 dataset, a dataset of 60,000 images divided into 10 categories for our model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_img, y_train_img), (X_test_img, y_test_img) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train_img = keras.utils.to_categorical(y_train_img)\n",
    "y_test_img = keras.utils.to_categorical(y_test_img)\n",
    "\n",
    "image_size = (X_train_img.shape[1], \n",
    "              X_train_img.shape[2], \n",
    "              X_train_img.shape[3])\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "for i in range(2):\n",
    "    axes[i].imshow(X_train_img[i])\n",
    "    axes[i].set_title(y_train_img[i])\n",
    "    \n",
    "fig.suptitle('First Two Images in Cifar10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Pretrained Layers\n",
    "\n",
    "\n",
    "## VGG1\n",
    "\n",
    "[VGG16](https://neurohive.io/en/popular-networks/vgg16/) is a very deep image classification model architecture.  \n",
    "\n",
    "It starts with a set input shape, (224,224, 3).  This represents a 2 dimensional RGB image (first two dimensions) and 3 color channels (last dimension).\n",
    "\n",
    "It uses convolutional and max pooling layers, but don't worry about what these are yet.  We will learn more about these soon.\n",
    "\n",
    "We will be setting the weights of this model to a collection of pre-trained weights called 'imagenet'.  These weights are the result of the VGG16 model's training on the [imagenet dataset](https://en.wikipedia.org/wiki/ImageNet).  This is a database of 14 million images in 20,000 different categories.  \n",
    "\n",
    "VGG19: https://keras.io/api/applications/vgg/#vgg19-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the VGG16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "pretrained = VGG16(weights='imagenet', \n",
    "                 include_top=False, \n",
    "                 input_shape=image_size)\n",
    "\n",
    "pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_transfer = Sequential()\n",
    "cnn_transfer.add(pretrained)\n",
    "\n",
    "# freezing layers so they don't get retrained with your new data\n",
    "for layer in cnn_transfer.layers:\n",
    "    layer.trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our own dense layers\n",
    "cnn_transfer.add(layers.Flatten())\n",
    "cnn_transfer.add(layers.Dense(132, activation='relu'))\n",
    "cnn_transfer.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify that the weights are \"frozen\" \n",
    "for layer in cnn_transfer.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_transfer.compile(loss='categorical_crossentropy', \n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "cnn_transfer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_custom = Sequential()\n",
    "cnn_custom.add(layers.Input(shape=image_size))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.MaxPool2D())\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
    "cnn_custom.add(layers.Flatten())\n",
    "cnn_custom.add(layers.Dense(132, activation='relu'))\n",
    "cnn_custom.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_custom.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "cnn_custom.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test each:\n",
    "\n",
    "Notice that the custom model is much shallower than VGG16, but has more trainable weights.  Take note of accuracy and training time between the two models.\n",
    "\n",
    "Transfer learning is not always the most successful tactic, but it tends to reduct training time and is sometimes more successful.  It is one more tool for the toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#fit and evaluate the custom CNN.\n",
    "cnn_custom.fit(X_train_img, y_train_img,\n",
    "               epochs=1,\n",
    "               batch_size=1024,\n",
    "               validation_split=.2)\n",
    "custom_loss, custom_accuracy = cnn_custom.evaluate(X_test_img, y_test_img)\n",
    "\n",
    "print(f'Loss of custom model on test set: {custom_loss:.2f}')\n",
    "print(f'Accuracy of custom model on test set: {custom_accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fit and evaluate the CNN using transfer learning.\n",
    "cnn_transfer.fit(X_train_img, y_train_img,\n",
    "                 epochs=1,\n",
    "                 batch_size=1024,\n",
    "                 validation_split=.2)\n",
    "transfer_loss, transfer_accuracy = cnn_transfer.evaluate(X_test_img, y_test_img)\n",
    "print(f'Loss of transfer model on test set: {transfer_loss:.2f}')\n",
    "print(f'Accuracy of transfer model on test set: {transfer_accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "1. Used Talos to search for optimal hyperparameters for a multi-layered perceptron with dense and dropout layers.\n",
    "2. Saved the tuned model and reloaded it using saved weights.\n",
    "3. Used transfer learning to reduce the training time for an image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.173px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
