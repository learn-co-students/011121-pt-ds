{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "intro_to_cnns.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "learn-env",
      "language": "python",
      "name": "learn-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4HI2mpwlrcn"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "679Lmwt3l1Bk"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# This work has been substantially altered from the original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPCom-KmApV"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVs_fMj4fLKg"
      },
      "source": [
        "Much of the code in this notebook comes from [tensorflow's GitHub](https://github.com/tensorflow/docs/tree/master/site/en/tutorials)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU2FoKZifLKg"
      },
      "source": [
        "## Goals for Today\n",
        "\n",
        "- Describe the types of layers that are distinctive for convolutional nets\n",
        "- Utilize `tensorflow` to build CNNs\n",
        "- Evaluate CNN models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0aUeDr9fLKh"
      },
      "source": [
        "## What are CNNs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4quYEXSWfLKh"
      },
      "source": [
        "From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):\n",
        "\n",
        "- \"CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns.\"\n",
        "<br/>\n",
        "<br/>\n",
        "- \"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw-_dFrRfLKh"
      },
      "source": [
        "### Convolving and Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_K8wJuUfLKh"
      },
      "source": [
        "The two distinctive types of layer inside of a typical CNN are **convolutional** and **pooling** layers. There often several of each of these layers in a network.  Let's look at each in turn.\n",
        "\n",
        "\n",
        "#### Convolution\n",
        "\n",
        "\n",
        "Convolutional nets employ [convolutions](https://en.wikipedia.org/wiki/Convolution), which are a certain kind of transformation. In the context of neural networks processing images, this can be thought of as sliding a filter (of weights) over the image matrix to produce a new matrix of values. (I'll detail the calculation below.) The relative smallness of the filter means both that there will be relatively few parameters to learn and that the values representing certain areas of the image will be affected only by the values of *nearby areas*. This helps the network in **feature detection**. Let's check out some visualizations [here](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/).\n",
        "\n",
        "![sliding window](https://github.com/learn-co-students/011121-pt-ds/blob/main/phase_4/resources/conv-layer-animation.gif?raw=1)\n",
        "*image thanks to [Daphne Cornelisse](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/)\n",
        "\n",
        "\n",
        "Utkarsh Sinha shows us some examples of different kinds of filters [here](http://aishack.in/tutorials/convolutions/).\n",
        "\n",
        "#### Hyper parameters:\n",
        "\n",
        "There are 3 important hyperparameters to consider when designing your convolutional layers:\n",
        "1. Filter Dimensions\n",
        "2. Stride\n",
        "3. Padding\n",
        "\n",
        "#### 1. Filter Dimensions:\n",
        "\n",
        "These are simply the shape of your filter.  (2,2) would be a 2 by 2 filter covering 4 total pixels.  (3,3) 3 by 3 and cover 9 pixels.  No matter the size, the filter will return a single value on each step.  The math is shown below.  Also, color images have 3 'channels' or layers, Red, Blue, and Green pixel values.  So, a 3x3 filter for a color image would actually have size 3x3x3.\n",
        "\n",
        "#### 2. Stride\n",
        "\n",
        "Stride is how far the filter moves with each step.  A stride of 1 means the filter moves over by 1 pixel on each step, the animation above demonstrates a 3x3 filter with a stride of 1.  A filter with a stride of 2 would move 2 pixels on each step.\n",
        "\n",
        "#### 3. Padding.  \n",
        "\n",
        "Padding is technically not a feature of the filter, but is preparation of the data before the filter is passed over it.  A convolutional layer can add a layer of zeros aorund the edge of the image to change the size of the output matrix.  \n",
        "\n",
        "**Valid padding** means no padding.  The layer will output a smaller matrix than the input.\n",
        "\n",
        "**Same padding** will output a matrix of the same size as the input matrix.  This is done by adding a layer of zeros all the way around the image of size `filter // 2` or filter size divided by 2, rounded down.  \n",
        "\n",
        "**Full padding** adds a layer of zeros of size `filter - 1`.  This will output a matrix larger than in input and each pixel will be in each position of the filter kernel.\n",
        "\n",
        "![padding](https://github.com/learn-co-students/011121-pt-ds/blob/main/phase_4/resources/padding.jpeg?raw=1)\n",
        "\n",
        "\n",
        "### The Math:\n",
        "\n",
        "\n",
        "Suppose we have a 3x3 image with no padding and a 2x2 filter and a stride of 1. Then the calculation goes as follows:\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "a & b & c \\\\\n",
        "d & e & g \\\\\n",
        "h & j & k\n",
        "\\end{bmatrix} *\n",
        "\\begin{bmatrix}\n",
        "f_1 & f_2 \\\\\n",
        "f_3 & f_4\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "f_1a + f_2b + f_3d + f_4e & f_1b + f_2c + f_3e + f_4g \\\\\n",
        "f_1d + f_2e + f_3h + f_4j & f_1e + f_2g + f_3j + f_4k\n",
        "\\end{bmatrix}$.\n",
        "\n",
        "In words: Line up the filter with the image, multiply all the corresponding pairs and then add up those products. Repeat for all positions of the filter as allowed by [the stride and the padding](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/). The relative position of the filter to the image will tell you which entry in the resultant matrix you're filling in.\n",
        "\n",
        "Let's try an example of horizontal edge detection. One good filter for that might look like:\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "10 & 10 & 10 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "-10 & -10 & -10\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Suppose we apply this filter to (i.e. *convolve*) an image with a clear horizontal edge, such as this one:\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "200 & 200 & 200 & 200 & 200 \\\\\n",
        "200 & 200 & 200 & 200 & 200 \\\\\n",
        "200 & 200 & 200 & 200 & 200 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}$\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "<details><summary>\n",
        "    Answer here\n",
        "    </summary>\n",
        "    <br/>\n",
        "    $\\begin{bmatrix}\n",
        "    0 & 0 & 0 \\\\\n",
        "    6000 & 6000 & 6000 \\\\\n",
        "    6000 & 6000 & 6000 \\\\\n",
        "    0 & 0 & 0\n",
        "    \\end{bmatrix}$\n",
        "    Notice how the edge is now \"highlighted\"!\n",
        "    </details>\n",
        "\n",
        "[Here](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) is another good resource.\n",
        "\n",
        "#### Pooling\n",
        "What is pooling? The main goal in inserting a pooling layer is to reduce dimensionality, which helps to reduce both network computation and model overfitting. This is generally a matter of reducing a matrix or tensor of values to  some smaller size, and the most common way of doing this is by partitioning the large matrix into $n$ x $n$ blocks and then replacing each with the largest value in the block. Hence we speak of \"MaxPooling\".\n",
        "\n",
        "### Max Pooling in Action\n",
        "\n",
        "![max pooling example](https://github.com/learn-co-students/011121-pt-ds/blob/main/phase_4/resources/maxpool.png?raw=1)\n",
        "\n",
        "#### Putting it all together\n",
        "\n",
        "The combination of convolutional layers and pooling layers compress two dimensions of the image while deepening the last through the repeated application of *multiple filters* per layer.  \n",
        "\n",
        "Each of the multiple different filters, or kernels, on each layer produce their own output which is generally smaller (in the case of valid padding) in \"width\" and \"height\" and the layer stacks the output of each filter to make the 3rd dimension of the matrix, \"depth\" greater.  \n",
        "\n",
        "Pooling layers further compress the \"height\" and \"width\" by pulling out the most relevant features of the image, those with the greatest activation value.\n",
        "\n",
        "Densely connected layers are generally added in the last few layers in image classification tasks.\n",
        "\n",
        "![CNN image classification example](https://github.com/learn-co-students/011121-pt-ds/blob/main/phase_4/resources/cnn_architecture_example.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX_XDTlkfLKi"
      },
      "source": [
        "## From the TensorFlow Authors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLGkt5qiyz4E"
      },
      "source": [
        "This tutorial demonstrates training a simple [Convolutional Neural Network](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network) (CNN) to classify MNIST digits. This simple network will achieve over 99% accuracy on the MNIST test set. Because this tutorial uses the [Keras Sequential API](https://www.tensorflow.org/guide/keras), creating and training our model will take just a few lines of code.\n",
        "\n",
        "Note: CNNs train faster with a GPU. If you are running this notebook with Colab, you can enable the free GPU via * Edit -> Notebook settings -> Hardware accelerator -> GPU*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7KBpffWzlxH"
      },
      "source": [
        "### Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CGHk1VrfLKj"
      },
      "source": [
        "#!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRFxccghyMVo"
      },
      "source": [
        "### Download and prepare the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWoEqyMuXFF4"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydXPMOFfLKk"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeKN20qFiaeF"
      },
      "source": [
        "fig, axes = plt.subplots(3,2, figsize=(10,5))\n",
        "axes = axes.ravel()\n",
        "for i in range(6):\n",
        "  axes[i].imshow(train_images[i].reshape((28,28)))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XQTbbYfLKk"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31MAynskfLKk"
      },
      "source": [
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255, test_images / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "### Create the convolutional base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hQvqXpNyN3x"
      },
      "source": [
        "The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to color channels, MNIST has one (because the images are grayscale), whereas a color image has three (R,G,B). In this example, we will configure our CNN to process inputs of shape (28, 28, 1), which is the format of MNIST images. We do this by passing the argument `input_shape` to our first layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9YmGQBQPrdn"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvDVFkg-2DPm"
      },
      "source": [
        "Let display the architecture of our model so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-C4XBg4UTJy",
        "scrolled": true
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJPLvi-mfLKl"
      },
      "source": [
        "The number of parameters depends on the number of input and output channels of the layer in question. For more, see [this post](https://medium.com/@zhang_yang/number-of-parameters-in-dense-and-convolutional-neural-networks-34b54c2ec349)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-AXYeZ2GO5"
      },
      "source": [
        "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, we can afford (computationally) to add more output channels in each Conv2D layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v8sVOtG37bT"
      },
      "source": [
        "### Add Dense layers on top\n",
        "To complete our model, we will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, we will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. MNIST has 10 output classes, so we use a final Dense layer with 10 outputs and a softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRs95d6LUVEi"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipGiQMcR4Gtq"
      },
      "source": [
        " Here's the complete architecture of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yu_m-TZUWGX"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNKXi-Gy3RO-"
      },
      "source": [
        "As you can see, our (3, 3, 64) outputs from the final max pooling layer were flattened into vectors of shape (576) before going through two Dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3odqfHP4M67"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfLewdbouMbw"
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdDzI75PUXrG"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHLL-IWfLKn"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKgyC5K_4O0d"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtyDF0MKUcM7"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LvwaKhtUdOo"
      },
      "source": [
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfJ8AR03gT5"
      },
      "source": [
        "As you can see, our simple CNN has achieved a really high test accuracy. Not bad for a few lines of code!\n",
        "\n",
        "If you would like to go further with this, including learning how to incrase the size of your image dataset with new, synthetic images using image augmentation, and how to train on more images than you can fit in your RAM, check out [this colab notebook by the team at Google](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part2.ipynb#scrollTo=SdW6geEVi2S8)\n",
        "\n",
        "# Conclusion:\n",
        "\n",
        "In this notebook you learned about convolutional neural networks.  \n",
        "\n",
        "You learned about their distinctive layers: convolutional and pooling, and how tensor data flows through them and changes shape.\n",
        "\n",
        "You also learned how to create an CNN in Keras to classify handwritten numerals with high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd3JIpDCzvCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}