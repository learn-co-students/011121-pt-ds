{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "RNNs and Word Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjVB2rMrXmeG"
      },
      "source": [
        "# Recurrent Neural Networks for Text Classification\n",
        "\n",
        "## Learning Objectives:\n",
        "1. Understand how a recurrent neural network differs from a feed forward network\n",
        "2. Learn to implement RNNs in Keras\n",
        "3. Explore word embeddings for natural language processing\n",
        "4. Use NLP to classify documents using word embeddings\n",
        "\n",
        "What is a Recurrent Neural Network?\n",
        "\n",
        "![RNN Image](https://www.researchgate.net/profile/Ali-Daud/publication/330920746/figure/fig2/AS:741304577318912@1553752192108/Folded-and-unfolded-regular-recurrent-neural-networks-with-single-hidden-recurrent-layer.png)\n",
        "\n",
        "Source: https://www.researchgate.net/figure/Recurrent-neural-networks-structure_fig4_326884617\n",
        "\n",
        "In a nutshell, an RNN takes in X<sub>t</sub> and outputs 2 things:  \n",
        "\n",
        "1. y<sub>t</sub> The output.\n",
        "2. W<sub>t</sub> a value that is passed back into the layer and combined with X<sub>t+1</sub> to produce:\n",
        "  1. y<sub>t+t</sub>\n",
        "  2. W<sub>t+1</sub>\n",
        "\n",
        "...and so on until the end of the sequence of the input.\n",
        "\n",
        "It is a recurring process that acts on each feature of an input **In Sequence**.\n",
        "\n",
        "The W value allows the model to pass information from previous X<sub>t-1</sub> inputs to affect how the model processes the X<sub>t<sub> input.\n",
        "\n",
        "You can think of this as a single node that is calling itself over and over (most accurate) or as a series of identical nodes being called in sequence (easier, but less accurate conceptualization).\n",
        "\n",
        "The values passed to the next layer in the model could consist of all of the Y values or just the final output of node after processing all of the inputs.  This is a hyperparameter you can tune in your model development.  Y output sequences can also make up final model outputs where a full sequence is desired, not just a classification.  Translation models would be an example of this impelementation.  Forecasting a time series would be another.\n",
        "\n",
        "There are many flavors of RNNs and what sets them apart is how the W is formed and what information is passed between recurrences.  \n",
        "\n",
        "*Note* Just a like a densely connected layer, information is potentially passed through multiple of these recurring nodes in parallel for each layer in the model.\n",
        "\n",
        "![multi-node RNN](https://gblobscdn.gitbook.com/assets%2F-LvBP1svpACTB1R1x_U4%2F-LwEQnQw8wHRB6_2zYtG%2F-LwEZT8zd07mLDuaQZwy%2Fimage.png?alt=media&token=93a3c3e2-e32b-4fec-baf5-5e6b092920c4)\n",
        "\n",
        "Source: https://docs.paperspace.com/machine-learning/wiki/recurrent-neural-network-rnn\n",
        "\n",
        "# Why RNNs?\n",
        "\n",
        "While a feed forward network of dense layers processes each feature independently, an RNN treats them as a sequence in which the order of features is important.  This makes it ideal for:\n",
        "\n",
        "1. Time series data\n",
        "2. Audio data\n",
        "3. Natural language\n",
        "4. Anything else where the order of features carries important information.\n",
        "\n",
        "# Memory Cells:\n",
        "\n",
        "LSTMs, or Long-Short Term Memory RNNS and GRUs, Gated Recurrent Units, pass an additional value with the W, a memory cell.  While the W is fully dependent on the X and the previous W<sub>t-1</sub> values, the memory cell has potentially persistent values through multiple loops.  This allows the model have a more persistent memory of inputs from many steps previous to X<sub>t</sub>.  \n",
        "\n",
        "For example, a GRU might learn to remember the tense of a sentence or the gender of a character.\n",
        "\n",
        "![RNNs, LSTMs, and GRUs](https://slidetodoc.com/presentation_image_h/8a65d4524f55935cb441b783479395ed/image-33.jpg)\n",
        "\n",
        "Source: https://slidetodoc.com/recurrent-neural-networks-deeplearning-ai-gated-recurrent-unit/\n",
        "\n",
        "# What are the trainable Parameters:\n",
        "\n",
        "The trainable parameters and activation functions in an RNN are the various weights and functions are inside each individual parallel node.  There are, potentially, more than one weight depending on the flavor of RNN.  However, the same weights are used for each feature in the sequence for a given node.\n",
        "\n",
        "This is different than a dense node where each feature has a separate weight.  One potential benefit of this is that certain configurations of RNNs can process **variable** length inputs, which feed forward networks cannot.\n",
        "\n",
        "#Further Reading\n",
        "\n",
        "Here is a wonderful article by Michael Phi with animations that does a great job explaining further:  https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
      ],
      "id": "AjVB2rMrXmeG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMsbEp8ujC4T"
      },
      "source": [
        "# LSTMs in Keras\n",
        "\n",
        "Keras has a pretty simple implementation of RNN layers.  Below I will demonstrate how to create an LSTM model."
      ],
      "id": "OMsbEp8ujC4T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK5WW8xPjTc8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, InputLayer, LSTM, \\\n",
        "Flatten, GlobalMaxPool1D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
      ],
      "id": "PK5WW8xPjTc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY1E3r_JxnYl"
      },
      "source": [
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"\n",
        "    Given a model history will plot the model training history and \n",
        "    return the last scores for each loss and metric in the  model.\n",
        "    Returns None.\n",
        "    \"\"\"\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    ax = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    loss = hist.columns[hist.columns.str.endswith('loss')]\n",
        "    accuracy = hist.columns[hist.columns.str.endswith('accuracy')]\n",
        "    hist[loss].plot(title='Loss', ax=ax)\n",
        "    hist[accuracy].plot(title='accuracy', ax=ax2)\n",
        "    plt.show()\n",
        "    \n",
        "    for l in loss:\n",
        "        print(f'final {l}: {hist[l].iloc[-1]}')\n",
        "    for r in accuracy:\n",
        "        print(f'final {r}: {hist[r].iloc[-1]}')\n",
        "    plt.show()"
      ],
      "id": "FY1E3r_JxnYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HdEBMQDj9I5"
      },
      "source": [
        "## The Data:\n",
        "\n",
        "We are going to return to our horror author data.  This is a collection of sentences from horror novels written by Edgar Allen Poe, H.P. Lovecraft, or Mary Shelley.  The task is to classify each sentency by the author who wrote it."
      ],
      "id": "6HdEBMQDj9I5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpgsvz78j2CM"
      },
      "source": [
        "#load the data \n",
        "spooky = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/011121-pt-ds/main/phase_4/nlp/horror_writers.csv', \n",
        "                     usecols=['text','author'])\n",
        "spooky.head()"
      ],
      "id": "Bpgsvz78j2CM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XNqdydroVcJ"
      },
      "source": [
        "X = spooky.text\n",
        "y = spooky.author\n",
        "\n",
        "#One hot encode labels for multiclass classification in Keras\n",
        "# lb = LabelBinarizer()\n",
        "# y= lb.fit_transform(y)\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "y = to_categorical(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n",
        "X_val, X_holdout, y_val, y_holdout = train_test_split(X_test, y_test, test_size = .5, random_state=123)"
      ],
      "id": "_XNqdydroVcJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzt0ix6slERg"
      },
      "source": [
        "## TextVectorization Layer\n",
        "\n",
        "We are going to use a special Keras layer called TextVectorization that will process our input for us and transform into a sequences of integers to pass to our LSTM layer.  Layers like this can make models more like pipelines.  Notice that we only fit the vectorizer layer on the training set to avoid data leakage."
      ],
      "id": "kzt0ix6slERg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_wR_yn7lfsJ"
      },
      "source": [
        "#max length of input.  \n",
        "#Will truncate inputs longer and add 0s to the end of sequences that are longer.\n",
        "output_sequence_length = spooky.text.str.len().max()\n",
        "\n",
        "vectorizer = TextVectorization(output_sequence_length=output_sequence_length,\n",
        "                                   )\n",
        "vectorizer.adapt(X_train.to_numpy())\n",
        "vocab_len = vectorizer.vocabulary_size()"
      ],
      "id": "F_wR_yn7lfsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLr0yZNIph3V"
      },
      "source": [
        "def create_LSTM():\n",
        "  model = Sequential()\n",
        "\n",
        "  #Define your input layer for one feature (the whole string) and the dtype.\n",
        "  #If you don't let Keras know to expect a string, it will assume it's looking\n",
        "  #for a float and you'll get an error.\n",
        "  model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "\n",
        "  #Fitted TextVectorization layer\n",
        "  model.add(vectorizer)\n",
        "\n",
        "  #Untrained Embedding Layer, embedding dimensions of 300\n",
        "  #We'll talk about this layer more a little farther down\n",
        "  model.add(Embedding(vocab_len, 300, input_length=output_sequence_length))\n",
        "  \n",
        "  #The recurrent LSTM layer.  We have it return all of its Y outputs for \n",
        "  #each cycle of each layer.\n",
        "  model.add(LSTM(50, return_sequences=True, dropout=0.5, \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)))\n",
        "  #The below below layer returns the highest activation of each LSTM node\n",
        "  #for each input and pass them to the dense layer.\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  \n",
        "  model.add(Dense(50, activation='relu', \n",
        "                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(50, activation='relu', \n",
        "                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  #Add an output layer.  3 nodes for 3 classes and a softmax activation\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  optimizer = optimizers.Adam(learning_rate=.01)\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "id": "oLr0yZNIph3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPjU9MA9q0gU"
      },
      "source": [
        "%%time\n",
        "model = create_LSTM()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data = (X_val, y_val),\n",
        "                    epochs = 10,\n",
        "                    batch_size = 128)\n"
      ],
      "id": "dPjU9MA9q0gU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgJB-sP94cUB"
      },
      "source": [
        "\n",
        "self_trained_score = model.evaluate(X_test, y_test)\n",
        "print(f'Accraucy on Test Set {self_trained_score[1]}, Loss: {self_trained_score[0]}')\n",
        "plot_history(history)"
      ],
      "id": "PgJB-sP94cUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndJGo_YRyKXq"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "You have have noticed that we used tf.keras.layers.Embedding() after the text vectorization layer.  This is an amazingly awesome way of encoding words.  It transforms each word from an integer (from the vectorizer layer) to a vector.  What does it mean for a word to be a vector?\n",
        "\n",
        "It means the representation of the word can contain arbitrary amounts of information about that word.  Each dimension of the space that the word vector exists in contains information about that word.\n",
        "\n",
        "We can determine relationships between words and define similarity.\n",
        "\n",
        "These embeddings are discovered using unsupervised learning on large datasets of text.  Several weights for embedding models are available that have been trained for thousands of hours on hundreds of millions of texts (All of wikipedia, or instance).  \n",
        "\n",
        "The results are a library of {word:vector} pairs with each dimension of the vector representing a relationship between words learned from the corpus the model was trained on.\n",
        "\n",
        "![Word embedding space](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)\n",
        "\n",
        "Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space\n"
      ],
      "id": "ndJGo_YRyKXq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mZFz4gOIETg"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "id": "8mZFz4gOIETg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJPneGqtJPIa"
      },
      "source": [
        "We can look up vectors like a dictionary"
      ],
      "id": "eJPneGqtJPIa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViGNdqogJKKd"
      },
      "source": [
        "word_vectors['coffee']"
      ],
      "id": "ViGNdqogJKKd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRTvK3iGJXnh"
      },
      "source": [
        "We can also use the gensim model to find relationships between words. For instance:\n",
        "\n",
        "1. Similarity"
      ],
      "id": "wRTvK3iGJXnh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc13A2tdJgZq"
      },
      "source": [
        "word_vectors.most_similar('coffee')"
      ],
      "id": "dc13A2tdJgZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813opERxKDrz"
      },
      "source": [
        "2. Analogies"
      ],
      "id": "813opERxKDrz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gyhvUBMKGMu"
      },
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = word_vectors.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result"
      ],
      "id": "7gyhvUBMKGMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-prlgPHJhqj"
      },
      "source": [
        "analogy('japan','japanese','america')"
      ],
      "id": "p-prlgPHJhqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWwGf6XNsjr5"
      },
      "source": [
        "analogy('man','king','woman')"
      ],
      "id": "FWwGf6XNsjr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wTc4cT8KwXj"
      },
      "source": [
        "# Bias in Word Embeddings\n",
        "These relationships are learned from texts from the real world and carry the associations present in them.  We need to be aware of introducing bias into models that may affect people's lives."
      ],
      "id": "0wTc4cT8KwXj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLOpkbWPK-il"
      },
      "source": [
        "analogy('man','doctor','woman')"
      ],
      "id": "oLOpkbWPK-il",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBnADOr4L5FZ"
      },
      "source": [
        "## Graphical Representation"
      ],
      "id": "SBnADOr4L5FZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2bX3oNIL2ws"
      },
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "        \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "id": "y2bX3oNIL2ws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5pP6EeL8eo"
      },
      "source": [
        "display_pca_scatterplot(word_vectors, \n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ],
      "id": "1Q5pP6EeL8eo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwWsn3T0MXBN"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "Gensim is great because it lets us easily explore word embeddings.  SpaCy is great because it loads word embeddings much faster, allowing us to use larger embedding models, and has some great functionality for NLP, including parts of speech, processing, and more."
      ],
      "id": "OwWsn3T0MXBN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGDz11fwMW6l"
      },
      "source": [
        ""
      ],
      "id": "HGDz11fwMW6l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1YqDnh_WSc7"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# download and import the large english model.\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "# nlp will be our word embedding model\n",
        "nlp = en_core_web_lg.load()\n"
      ],
      "id": "B1YqDnh_WSc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN8EfMTtM4AB"
      },
      "source": [
        "In our last model, we allowed our embedding layer to learn its own embedding vectors from the texts in our training corpus.  But what if we used transfer learning to instead load the weights from a pretrained word embedding model?"
      ],
      "id": "AN8EfMTtM4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96mMeBo1Nnaw"
      },
      "source": [
        "# Creating Word Embedding Weights\n",
        "\n",
        "We have to create our own matrix of weights for our embedding layer.  We do this by using our vectorizer to create an index for a matrix, and filling in each row of the matrix with the word embedding provided by our pretrained word embedding model.  The result is a (vocab, embedding_dimension) shaped numpy array that our LSTM layer can use as a lookup table to find the correct embedding for each word in our text samples.\n",
        "\n",
        "Finally, we freeze those weights to reduce computation time.\n",
        "\n",
        "## LSTM\n",
        "As you recall, each node of the LSTM layer processes each word in each document in order, using the word embedding from the Embedding layer as the value for that word.  LSTMs 'remember' information from previous items in the sequence, words in the sentence in this case, and carry that information forward to help process later items in the sequence.\n"
      ],
      "id": "96mMeBo1Nnaw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-bG1KljOqg5"
      },
      "source": [
        "\n",
        "#generate the embedding matrix\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "num_tokens = len(vocab)\n",
        "embedding_dim = len(nlp('The').vector)\n",
        "\n",
        "#Generate the embedding weights usign the SpaCy word vectors for the vocabulary\n",
        "#in our training corpus.  (This may take awhile)\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_matrix[i] = nlp(word).vector\n",
        "\n",
        "#Load the embedding matrix as the weights matrix for the embedding layer and set trainable to False\n",
        "Embedding_layer=Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=False)"
      ],
      "id": "Z-bG1KljOqg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEz7WbbFsVJ-"
      },
      "source": [
        "def create_pretrain_LSTM():\n",
        "  model = Sequential()\n",
        "\n",
        "  #Define your input layer for one feature (the whole string) and the dtype.\n",
        "  #If you don't let Keras know to expect a string, it will assume it's looking\n",
        "  #for a float and you'll get an error.\n",
        "  model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "\n",
        "  #Fitted TextVectorization layer\n",
        "  model.add(vectorizer)\n",
        "\n",
        "  #Untrained Embedding Layer, embedding dimensions of 300\n",
        "  #We'll talk about this layer more a little farther down\n",
        "  model.add(Embedding_layer)\n",
        "  \n",
        "  #The recurrent LSTM layer.  We have it return all of its Y outputs for \n",
        "  #each cycle of each layer.\n",
        "  model.add(LSTM(50, return_sequences=True, dropout=0.5, \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)))\n",
        "  #The below below layer returns the highest activation of each LSTM node\n",
        "  #for each input and pass them to the dense layer.\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  \n",
        "  model.add(Dense(50, activation='relu', \n",
        "                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(50, activation='relu', \n",
        "                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  #Add an output layer.  3 nodes for 3 classes and a softmax activation\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  optimizer = optimizers.Adam(learning_rate=.01)\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "id": "LEz7WbbFsVJ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qPHE-gTOd91"
      },
      "source": [
        "%%time\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = create_pretrain_LSTM()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data = (X_val, y_val),\n",
        "                    epochs = 10,\n",
        "                    batch_size = 128)\n",
        "\n",
        "pretrained_score = model.evaluate(X_test, y_test)\n",
        "\n"
      ],
      "id": "6qPHE-gTOd91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXAUhl0y4wBt"
      },
      "source": [
        "\n",
        "plot_history(history)\n",
        "print(f'Accuracy of Self-trained Embedding model: {self_trained_score[1]}, Loss: {self_trained_score[0]}')\n",
        "print(f'Accuracy of Pretrained Embedding model: {pretrained_score[1]}, Loss: {pretrained_score[0]}')"
      ],
      "id": "kXAUhl0y4wBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEfzz-qitaBw"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this lesson you learned about recurrent neural networks, or RNNs, how to implement them in Keras, and how to use transfer learning to improve a natural language processing modeling task.\n",
        "\n",
        "I hope you feel comfortable and confident to use these powerful tools in your own work.\n",
        "\n",
        "# References:\n",
        "\n",
        "[Pretrained Word Embeddings using SpaCy and Keras TextVectorization](https://towardsdatascience.com/pretrained-word-embeddings-using-spacy-and-keras-textvectorization-ef75ecd56360)\n",
        "\n",
        "[Multi-Class Text Classification with LSTM](https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17)"
      ],
      "id": "DEfzz-qitaBw"
    }
  ]
}